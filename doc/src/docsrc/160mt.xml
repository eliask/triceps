<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5CR3//EN"
	"http://www.oasis-open.org/docbook/xml/4.5CR3/docbookx.dtd" [
<!ENTITY % userents SYSTEM "file:///ENTS/user.ent" >
%userents;
]>

<!--
(C) Copyright 2011-2013 Sergey A. Babkin.
This file is a part of Triceps.
See the file COPYRIGHT for the copyright notice and license information
-->

<chapter id="ch_mt" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Multithreading</title>

	<sect1 id="sc_mt_concepts">
		<title>Triceps multithreading concepts</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<para>
		When running the CEP models, naturally the threads have to be connected
		by the queues for the data exchange. The use of queues is extremely
		popular but also notoriously bug-prone.
		</para>

		<para>
		The idea of the multithreading support in Triceps is to make writing
		the multithreaded model easier. To make writing the good code easy and
		writing the bad code hard. But of course you don't have to use it, if 
		it feels too constraining, you can always make your own.
		</para>

		<para>
		The diagram in
		<xref linkend="fig_mt_overview" xrefstyle="select: label nopage"/>&xrsp;
		shows all the main elements of a multithread Triceps application.
		</para>

		<figure id="fig_mt_overview" >
			<title>Triceps multithreaded application.</title>
			<xi:include href="file:///FIGS/thread-010-over.xml"/> 
		</figure>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<indexterm>
			<primary>fragment</primary>
		</indexterm>
		<para>
		The Triceps application is embodied in the class App. It's possible to
		have multiple Apps in one program.
		</para>

		<para>
		Each thread has multiple parts to it. First, of course, there is the
		OS-level (or, technically, library-level, or Perl-level) thread where
		the code executes. And then there is a class that represents this
		thread and its place in the App. To reduce the naming conflict, this
		class is creatively named Triead (pronounced still <quote>thread</quote>). In the
		discussion I use the word <quote>thread</quote> for both concepts, the OS-level
		thread and the Triead, and it's usually clear from the context which
		one I mean. But sometimes it's particularly important to make the
		distinction, and then I name one or the other explicitly.
		</para>

		<para>
		The class Triead itself is largely opaque, allowing only a few methods
		for introspection. But there is a control interface to it, called
		TrieadOwner. The Triead is visible from the outside, the TrieadOwner
		object is visible only in the OS thread that owns the Triead. The
		TrieadOwner manages the thread state and acts as the intermediary in
		the thread's communications with the App.
		</para>

		<para>
		The data is passed between the threads through the Nexuses. A Nexus is
		unidirectional, with data going only one way, however it may have
		multiple writers and multiple readers. All the readers see the exact
		same data, with rowops going in the exact same order (well, there will
		be other policies in the future as well, but for now there is only one
		policy).
		</para>

		<para>
		A Nexus passes through the data for multiple labels, very much like an
		FnReturn does (and indeed there is a special connection between them).
		A Nexus also allows to export the row types and table types from one
		thread to another.
		</para>

		<para>
		A Nexus is created by one thread, and then the other threads connect to it.
		The thread that creates the Nexus determines what labels will it contain,
		and what row types and table types to export.
		</para>

		<para>
		A Nexus gets connected to the Trieads through the Facets (in the diagram,
		the Facets are shown as flat spots on the round Nexuses). A Facet is
		a connection point between the Nexus and the Triead. Each Facet is for
		either reading or writing. And there may be only one Facet between a
		given Nexus and a given Triead, you can't make multiple connections
		between them. As a consequence, a thread can't both write and read to
		the same Nexus, it can do only one thing. This might actually be an
		overly restrictive limitation and might change in the future but that's
		how things work now.
		</para>

		<para>
		Each Nexus also has a direction: either direct (<quote>downwards</quote>) or reverse
		(<quote>upwards</quote>). How does it know, which direction is down and
		whih is up? It doesn't. You tell it by designating a Nexus one way or the other.
		And yes, the reverse Nexuses allow to build the models
		with loops. However the loops consisting of only the direct Nexuses are
		not allowed, nor of only reverse Nexuses. They would mess up the flow
		control. The proper loops must contain a mix of direct and reverse
		Nexuses.
		</para>

		<para>
		The direct Nexuses have a limited queue size and stop the writers when
		the queue fills up, until the data gets consumed, thus providing the
		flow control. The reverse Nexuses have an unlimited queue size, which
		allows to avoid the circular deadlocks. The reverse Nexuses also have
		a higher priority: if a thread is reading from a direct Nexus and a
		reverse one, with both having data available, it will read the data
		from the reverse Nexus first. This is to prevent the unlimited queues
		in the reverse Nexuses from the truly unlimited growth.
		</para>

		<para>
		Normally an App is built once and keeps running in this configuration
		until it stops. But there is a strong need to have the threads
		dynamically added and deleted too. For example, if the App running as a
		server, and clients connect to it, each client needs to have its
		thread(s) added when the client connects and then deleted when the client
		disconnects. This is handled through the concept of fragments. There is
		no Fragment class but when you create a Triead, you can specify a
		fragment name for it. Then it becomes possible to shut down and dispose
		the threads in a fragment after the fragment's work is done. 
		</para>
	</sect1>

	<sect1 id="sc_mt_triead_life">
		<title>The Triead lifecycle</title>

		<indexterm>
			<primary>Triead</primary>
			<secondary>stages</secondary>
		</indexterm>

		<para>
		Each Triead goes through a few stages in its life:
		</para>

		<itemizedlist>
			<listitem>
			declared
			</listitem>
			<listitem>
			defined
			</listitem>
			<listitem>
			constructed
			</listitem>
			<listitem>
			ready
			</listitem>
			<listitem>
			waited ready
			</listitem>
			<listitem>
			requested dead
			</listitem>
			<listitem>
			dead
			</listitem>
		</itemizedlist>

		<para>
		Note by the way that it's the stages of the Triead object. The OS-level
		thread as such doesn't know much about them, even though these stages
		do have some connections to its state.
		</para>

		<para>
		These stages always go in order and can not be skipped. However for
		convenience you can request a move directly to a further stage. This will just
		automatically pass through all the intermediate stages. Although, well,
		there is one exception: the <quote>waited ready</quote> and <quote>requested dead</quote> stages
		can get skipped on the way to <quote>dead</quote>. Other than that, there is always
		the sequence, so if you find out that a Triead is dead, you can be sure
		that it's also declared, defined, constructed and ready. The attempts
		to go to a previous stage are silently ignored.
		</para>

		<para>
		Now, what do these stages mean?
		</para>

		<variablelist>

			<varlistentry>
				<term>Declared:</term>
				<listitem>
				<para>
				The App knows the name of the thread and that this thread
				will eventually exist. When an App is asked to find the resources from
				this thread (such as Nexuses, and by the way, the Nexuses are
				associated with the threads that created them) it will know to wait
				until this thread becomes constructed, and then look for the resources.
				It closes an important race condition: the code that defines the Triead
				normally runs in a new OS thread but there is no way to tell when
				exactly will it run and do its work. If you had spawned a new thread and
				then attempted to get a nexus from it before it actually runs, the App
				would tell you that there is no such thread and fail. To get around it,
				you declare the thread first and then start it. Most of the time there
				is no need to declare explicitly, the library code that wraps the
				thread creation does it for you.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Defined:</term>
				<listitem>
				<para>
				The Triead object has been created and connected to the App.
				Since this is normally done from the new OS thread, it also implies
				that the thread is running and is busy about constructing the nexuses
				and whatever its own internal resources.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Constructed:</term>
				<listitem>
				<para>
				The Triead had constructed and exported all the nexuses
				that it planned to. This means that now these nexuses can be imported
				by the other threads (i.e. connected to the other threads). After this
				point the thread can not construct any more nexuses. However it can
				keep importing the nexuses from the other threads. It's actually a good
				idea to do all your exports, mark the thread constructed, and only then
				start importing. This order guarantees the absence of initialization deadlocks (which
				would be detected and will cause the App to be aborted). There are some
				special cases when you need to import a nexus from a thread that is not
				fully constructed yet, and it's possible, but requires more attention
				and a special override. I'll talk about it in more detail in 
				XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Ready:</term>
				<listitem>
				<para>
				The thread had imported all the nexuses it wanted and fully
				initialized all its internals (for example, if it needs to load data
				from a file, it might do that before telling that it's ready). After
				this point no more nexuses can be imported. A fine point is that the
				other threads may still be created, and they may do their exporting and
				importing, but once a thread is marked as ready, it's cast in bronze.
				And in the simple cases you don't need to worry about separating the
				constructed and ready stages, just initialize everything and mark the
				thread as ready.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Waited ready:</term>
				<listitem>
				<para>
				Before proceeding further, the thread has to wait for all
				the threads in App to be ready, or it would lose data when it tries to
				communicate with them. It's essentially a barrier. Normally both the
				stages <quote>ready</quote> and <quote>waited ready</quote> are advanced to with a single call
				<pre>readyReady()</pre>. With it the thread says <quote>I'm ready, and let me continue when
				everyone is ready</quote>. After that the actual work can begin. It's still
				possible to create more threads after that (normally, parts of the
				transient fragments), and until they all become ready, the App may
				temporarily become unready again, but that's a whole separate advanced
				topic that will be discussed in XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Requested dead:</term>
				<listitem>
				<para>
				This is the way to request a thread to exit. Normally
				some control thread will decide that the App needs to exit and will
				request all its threads to die. The threads will get these requests,
				perform their last rites and exit. The threads don't have to get this
				request to exit, they can also always decide to exit on their own. When
				a thread is requested to die, all the data communication with it stops.
				No more data will get to it through the nexuses and any data it sends
				will be discarded. It might churn a little bit through the data in its
				input buffers but any results produced will be discarded. The good
				practice is to make sure that all the data is drained before requesting
				a thread to die. Note that the nexuses created by this thread aren't
				affected at all, they keep working as usual. It's the data connections
				between this thread and any nexuses that get broken.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Dead:</term>
				<listitem>
				<para>
				The thread had completed its execution and exited. Normally you
				don't need to mark this explicitly. When the thread's main function
				returns, the library will do it for you. Marking the thread dead also
				drives the harvesting of the OS threads: the harvesting logic will
				perform a <pre>join()</pre> (not to be confused with SQL join) of the thread and
				thus free the OS resources. The dead Trieads are still visible in the
				App (except for some special cases with the fragments), and their
				nexuses continue working as usual (even including the special cases
				with the fragments), the other threads can keep communicating through
				them for as long as they want. 
				</para>
				</listitem>
			</varlistentry>
		</variablelist>
	</sect1>

	<sect1 id="sc_mt_pipeline">
		<title>Multithreaded pipeline</title>

		<indexterm>
			<primary>pipeline</primary>
		</indexterm>
		<para>
		The multithreaded models are well suited for running the pipelines,
		so that is going to be the first example of the threads. The full text of the example
		can be found in <pre>t/xTrafficAggMt.t</pre> in the class Traffic1.
		It's a variation of an already shown example, the traffic
		data aggregation from 
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;.
		The short recap is that it gets
		the data for each network packet going through and keeps it for some
		time, aggregates the data by the hour and keeps it for a longer time,
		and aggregates it by the day and keeps for a longer time yet. This
		multi-stage computation naturally matches the pipeline approach.
		</para>

		<para>
		Since this new example highlights different features than the original one,
		I've changed it logic a little: it updates both the
		hourly and daily summaries on every packet received. And I didn't
		bother to implement the part with the automatic cleaning of the old
		data, it doesn't add anything interesting to the pipeline works.
		</para>

		<para>
		The pipeline topologies are quite convenient for working with the
		threads. The parallel computations create a possibility of things
		happening in an unpredictable order and producing unpredictable
		results. The pipeline topology allows the parallelism and at the same
		time also keeps the data in the same predictable order, with no
		possibility of rows overtaking each other.
		</para>

		<para>
		The computation in this example is split into the following threads:
		</para>

		<itemizedlist>
			<listitem>
			Read the input, parse and send the data into the model.
			</listitem>

			<listitem>
			Store the recent data and aggregate it by the hour.
			</listitem>

			<listitem>
			Store the hourly data and aggregate it by the day.
			</listitem>

			<listitem>
			Store the daily data.
			</listitem>

			<listitem>
			Get the data at the end of the pipeline and print it.
			</listitem>
		</itemizedlist>

		<para>
		The result of each aggregation gets stored in a table in the next thread,
		which then uses the same table for the next stage of aggregation.
		</para>

		<para>
		Technically, each stage only needs the data from the previous
		stage, but to get the updates to the printing stage (since we want
		to print the original updates, daily and hourly), they all go all
		the way through.
		</para>

		<para>
		Dumping the contents of the tables also requires some special support.
		Each table is local to its thread and can't be accessed from the other
		threads. To dump its contents, the dump request needs to be sent to its
		thread, which would extract the data and send it through. There are
		multiple ways to deal with the dump results. One is to have a special
		label for each table's dump and propagate it to the last stage to
		print. If all that is needed is text, another way is to have one label that allows to send
		strings is good enough, all the dumps can send the data converted to
		text into it, and it would go all the way through the pipeline.
		For this example I've picked the last approach.
		</para>

		<para>
		And now is time to show some code. The main part goes like this:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
Triceps::Triead::startHere(
	app => "traffic",
	thread => "print",
	main => \&printT,
);
</pre>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<para>
		The <pre>startHere()</pre> creates an App and starts a Triead in the current OS
		thread. <quote>Here</quote> in the method name stands for <quote>in the current OS thread</quote>.
		<quote>traffic</quote> is the app name, <quote>print</quote> the thread name. This thread
		will be the end of the pipeline, and it will create the rest of the
		threads. This is a convenient pattern when the results of the model
		need to be fed back to the current thread, and it works out very
		conveniently for the unit tests. <pre>printT()</pre> is the body function of
		this printing thread:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub printT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	Triceps::Triead::start(
		app => $opts->{app},
		thread => "read",
		main => \&readerT,
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "raw_hour",
		main => \&rawToHourlyT,
		from => "read/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "hour_day",
		main => \&hourlyToDailyT,
		from => "raw_hour/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "day",
		main => \&storeDailyT,
		from => "hour_day/data",
	);

	my $faIn = $owner->importNexus(
		from => "day/data",
		as => "input",
		import => "reader",
	);

	$faIn->getLabel("print")->makeChained("print", undef, sub {
		&send($_[1]->getRow()->get("text"));
	});
	for my $tag ("packet", "hourly", "daily") {
		makePrintLabel($tag, $faIn->getLabel($tag));
	}

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		<pre>startHere()</pre> accepts a number of fixed options plus arbitrary options
		that it doesn't care about by itself but passes through to the thread's main
		function, which are then the responsibility of the main function to
		parse. To reiterate, the main function gets all the options from the
		call of <pre>startHere()</pre>, both these that <pre>startHere()</pre> parses and these that
		it simply passes through. <pre>startHere()</pre> also adds one more option on its
		own: <quote>owner</quote> containing the TrieadOwner object that the thread uses to
		communicate with the rest of the App.
		</para>

		<para>
		In this case <pre>printT()</pre> doesn't have any extra options on its own,
		it's just happy to get <pre>startHere()</pre>'s standard set that it takes all
		together from <pre>@Triceps::Triead::opts</pre>.
		</para>

		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Unit</primary>
		</indexterm>
		<indexterm>
			<primary>UnitClearingTrigger</primary>
		</indexterm>
		<para>
		It gets the TrieadOwner object <pre>$owner</pre> from the option appended by
		<pre>startHere()</pre>. Each TrieadOwner is created with its own Unit, so the unit
		is obtained from it to create the thread's model in it. Incidentally,
		the TrieadOwner  also acts as a clearing trigger object for the Unit,
		so when the TrieadOwner is destroyed, it properly clears the Unit.
		</para>

		<para>
		Then it goes and creates all the threads of the pipeline. The <pre>start()</pre>
		works very much like <pre>startHere()</pre>, only it actually creates a new thread
		and starts the main function in it. The main function can be the same
		whether it runs through <pre>start()</pre> or <pre>startHere()</pre>. The special catch is
		that the options to <pre>start()</pre> must contain only the plain Perl values,
		not Triceps objects. It has to do with how Perl works with threads: it
		makes a copy of every value for the new thread, and it cant's copy the
		XS objects, so they simply become undefined in the new thread.
		</para>

		<para>
		All but the first thread in the pipeline have the extra option <quote>from</quote>: it
		specifies the input nexus for this thread, and each thread creates an
		output nexus <quote>data</quote>. A nexus it named relatively to the
		thread that created it, so when the option <quote>from</quote> says <quote>day/data</quote>, it's
		the nexus <quote>data</quote> created by the thread <quote>day</quote>.
		</para>

		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<para>
		So, the pipeline gets all connected sequentially until eventually
		<pre>printT()</pre> imports the nexus at its tail. <pre>importNexus()</pre> returns a
		Facet, which is the thread's API to the nexus. A facet looks very much
		like an FnReturn for most purposes, with a few additions. It even has a
		real FnReturn in it, and you work with the labels of that FnReturn to get the
		data out of the nexus (or to send data into the nexus). You could potentially
		use an FnBinding with that FnReturn but the typical pattern for reading
		from a facet is different: just get its labels 
		and chain the handling labels directly to them.
		</para>

		<para>
		The option <quote>as</quote> of <pre>importNexus()</pre> gives the name to the facet and to its
		same-named FnReturn (without it the facet would be named the same as
		the short name of the nexus, in this case <quote>data</quote>). The option <quote>import</quote>
		tells whether this thread will be reading or writing the nexus, and
		in this case it's reading.
		</para>

		<para>
		By the time the pipeline gets to the last stage, it has a few
		labels in its facet:
		</para>

		<itemizedlist>
			<listitem>
			<pre>print</pre> - carries the direct text lines to print in its field <pre>text</pre>,
			and its contents gets printed.
			</listitem>

			<listitem>
			<pre>dumprq</pre> - carries the dump requests to the tables, and the printing
			thread doesn't care about it.
			</listitem>

			<listitem>
			<pre>packet</pre> - carries the raw data about the packets.
			</listitem>

			<listitem>
			<pre>hourly</pre> - carries the hourly summaries.
			</listitem>

			<listitem>
			<pre>daily</pre> - carries the daily summaries.
			</listitem>
		</itemizedlist>

		<para>
		The last three get also printed but this time as whole rows.
		</para>

		<para>
		And after everything is connected, the thread both tells that it's
		ready and waits for all the other threads to become ready by calling
		<pre>readyReady()</pre>. Then its the run time, and <pre>mainLoop()</pre> takes care of it:
		it keeps reading data from the nexus and
		processes it until it's told to shutdown.
		The shutdown will be controlled by the file reading
		thread at the start of the pipeline. The processing is done by getting
		the rowops from the nexus and calling them on the appropriate label in
		the facet, which then calls the the labels chained from it, and that
		gets all the rest of the thread's model running.
		</para>

		<para>
		The reader thread drives the pipeline:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub readerT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	my $rtPacket = Triceps::RowType->new(
		time => "int64", # packet's timestamp, microseconds
		local_ip => "string", # string to make easier to read
		remote_ip => "string", # string to make easier to read
		local_port => "int32", 
		remote_port => "int32",
		bytes => "int32", # size of the packet
	);

	my $rtPrint = Triceps::RowType->new(
		text => "string", # the text to print (including \n)
	);

	my $rtDumprq = Triceps::RowType->new(
		what => "string", # identifies, what to dump
	);

	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			packet => $rtPacket,
			print => $rtPrint,
			dumprq => $rtDumprq,
		],
		import => "writer",
	);

	my $lbPacket = $faOut->getLabel("packet");
	my $lbPrint = $faOut->getLabel("print");
	my $lbDumprq = $faOut->getLabel("dumprq");

	$owner->readyReady();

	while(&readLine) {
		chomp;
		# print the input line, as a debugging exercise
		$unit->makeArrayCall($lbPrint, "OP_INSERT", "> $_\n");

		my @data = split(/,/); # starts with a command, then string opcode
		my $type = shift @data;
		if ($type eq "new") {
			$unit->makeArrayCall($lbPacket, @data);
		} elsif ($type eq "dump") {
			$unit->makeArrayCall($lbDumprq, "OP_INSERT", $data[0]);
		} else {
			$unit->makeArrayCall($lbPrint, "OP_INSERT", "Unknown command '$type'\n");
		}
		$owner->flushWriters();
	}

	{
		# drain the pipeline before shutting down
		my $ad = Triceps::AutoDrain::makeShared($owner);
		$owner->app()->shutdown();
	}
}
</pre>

		<para>
		It starts by creating the nexus with the initial set of the labels: for
		the data about the network packets, for the lines to be printed at the
		end of the pipeline and for the dump requests to the tables in the
		other threads. It gets exported for the other threads to import, and
		also imported right back into this thread, for writing. And then the
		setup is done, <pre>readyReady()</pre> is called, and the processing starts.
		</para>

		<para>
		It reads the CSV lines, splits them, makes a decision if it's a data
		line or dump request, and one way or the other sends it into the nexus.
		The data sent to a facet doesn't get immediately forwarded to the
		nexus. It's collected internally in a tray, and then <pre>flushWriters()</pre>
		sends it on. The <pre>mainLoop()</pre> shown in <pre>printT</pre> calls <pre>flushWriters()</pre>
		automatically after every tray it processes from the input. But when
		reading from a file you've got to do it yourself. Of course, it's more
		efficient to send through multiple rows at once, so a smarter
		implementation would check if multiple lines are available from the
		file and send them in larger bundles.
		</para>

		<indexterm>
			<primary>shutdown</primary>
		</indexterm>
		<indexterm>
			<primary>drain</primary>
		</indexterm>
		<para>
		The last part is the shutdown. After the end of file is reached, it's
		time to shut down the application. You can't just shut down it right
		away because there still might be data in the pipeline, and if you shut
		it down, that data will be lost. The right way is to drain the pipeline
		first, and then do the shutdown when the app is drained.
		<pre>AutoDrain::makeShared()</pre> creates a scoped drain: the drain request for
		all the threads is started when this object is created, and the object
		construction completes when the drain succeeds. When the object is
		destroyed, that releases the drain. So in this case the drain succeeds and
		then the app gets shut down.
		</para>

		<para>
		The shutdown causes the <pre>mainLoop()</pre> calls in all the other threads to
		return, and the threads to exit. Then <pre>startHere()</pre> in the first thread
		has the special logic in it that joins all the started threads after
		its own main function returns and before it completes. After that the
		script continues on its way and is free to exit. 
		</para>

		<para>
		The rest of this example might be easier to understand by looking at an
		example of a run first. The lines in bold are the copies of
		the input lines that <pre>readerT()</pre> reads from the input and
		sends into the pipeline, and <pre>printT()</pre> faithfully
		prints.
		</para>

		<para>
		<pre>input.packet</pre> are the rows that reach the <pre>printT</pre> on the <pre>packet</pre> label
		(remember, <quote>input</quote> is the name with which it imports its input nexus).
		<pre>input.hourly</pre> is the data aggregated by the hour intervals (and also by
		the IP addresses, dropping the port information), and <pre>input.daily</pre>
		further aggregates it per day (and again per the IP addresses). The
		timestamps in the hourly and daily rows are truncated to the start
		of the hour or day.
		</para>

		<para>
		And the lines without any prefixes are the dumps of the table contents
		that again reach the <pre>printT()</pre> through the <quote>print</quote> label:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<exdump>
> new,OP_INSERT,1330886011000000,1.2.3.4,5.6.7.8,2000,80,100
input.packet OP_INSERT time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
> new,OP_INSERT,1330886012000000,1.2.3.4,5.6.7.8,2000,80,50
input.packet OP_INSERT time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
input.hourly OP_DELETE time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
> new,OP_INSERT,1330889612000000,1.2.3.4,5.6.7.8,2000,80,150
input.packet OP_INSERT time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
> new,OP_INSERT,1330889811000000,1.2.3.4,5.6.7.8,2000,80,300
input.packet OP_INSERT time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
input.hourly OP_DELETE time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
> new,OP_INSERT,1330972411000000,1.2.3.5,5.6.7.9,3000,80,200
input.packet OP_INSERT time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
input.hourly OP_INSERT time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
input.daily OP_INSERT time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> new,OP_INSERT,1331058811000000
input.packet OP_INSERT time="1331058811000000" 
> new,OP_INSERT,1331145211000000
input.packet OP_INSERT time="1331145211000000" 
> dump,packets
time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
> dump,hourly
time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> dump,daily
time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
</exdump>

		<para>
		Note that the order of the lines is completely nice and predictable,
		nothing goes out of order. Each nexus preserves the order of the rows
		put into it, and the fact that there is only one writer per nexus and
		that every thread is fed from only one nexus, avoids the races.
		</para>

		<para>
		Let's look at the thread that performs the aggregation by the hour:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 edited -->
<pre>
# compute an hour-rounded timestamp (in microseconds)
sub hourStamp # (time)
{
	return $_[0]  - ($_[0] % (1000*1000*3600));
}

sub rawToHourlyT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {
		@Triceps::Triead::opts,
		from => [ undef, \&Triceps::Opt::ck_mandatory ],
	}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	# The current hour stamp that keeps being updated;
	# any aggregated data will be propagated when it is in the
	# current hour (to avoid the propagation of the aggregator clearing).
	my $currentHour;

	my $faIn = $owner->importNexus(
		from => $opts->{from},
		as => "input",
		import => "reader",
	);

	# the full stats for the recent time
	my $ttPackets = Triceps::TableType->new($faIn->getLabel("packet")->getRowType())
		->addSubIndex("byHour", 
			Triceps::IndexType->newPerlSorted("byHour", undef, sub {
				return &hourStamp($_[0]->get("time")) <=> &hourStamp($_[1]->get("time"));
			})
			->addSubIndex("byIP", 
				Triceps::IndexType->newHashed(key => [ "local_ip", "remote_ip" ])
				->addSubIndex("group",
					Triceps::IndexType->newFifo()
				)
			)
		)
	;

	# type for a periodic summary, used for hourly, daily etc. updates
	my $rtSummary;

	Triceps::SimpleAggregator::make(
		tabType => $ttPackets,
		name => "hourly",
		idxPath => [ "byHour", "byIP", "group" ],
		result => [
			# time period's (here hour's) start timestamp, microseconds
			time => "int64", "last", sub {&hourStamp($_[0]->get("time"));},
			local_ip => "string", "last", sub {$_[0]->get("local_ip");},
			remote_ip => "string", "last", sub {$_[0]->get("remote_ip");},
			# bytes sent in a time period, here an hour
			bytes => "int64", "sum", sub {$_[0]->get("bytes");},
		],
		saveRowTypeTo => \$rtSummary,
	);

	$ttPackets->initialize();
	my $tPackets = $unit->makeTable($ttPackets, "tPackets");

	# Filter the aggregator output to match the current hour.
	my $lbHourlyFiltered = $unit->makeDummyLabel($rtSummary, "hourlyFiltered");
	$tPackets->getAggregatorLabel("hourly")->makeChained("hourlyFilter", undef, sub {
		if ($_[1]->getRow()->get("time") == $currentHour) {
			$unit->call($lbHourlyFiltered->adopt($_[1]));
		}
	});

	# update the notion of the current hour before the table
	$faIn->getLabel("packet")->makeChained("processPackets", undef, sub {
		my $row = $_[1]->getRow();
		$currentHour = &hourStamp($row->get("time"));
		# skip the timestamp updates without data
		if (defined $row->get("bytes")) {
			$unit->call($tPackets->getInputLabel()->adopt($_[1]));
		}
	});

	# The makeNexus default option chainFront => 1 will make
	# sure that the pass-through data propagates first, before the
	# processed data.
	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			$faIn->getFnReturn()->getLabelHash(),
			hourly => $lbHourlyFiltered,
		],
		import => "writer",
	);

	my $lbPrint = $faOut->getLabel("print");

	# the dump request processing
	$tPackets->getDumpLabel()->makeChained("printDump", undef, sub {
		$unit->makeArrayCall($lbPrint, "OP_INSERT", $_[1]->getRow()->printP() . "\n");
	});
	$faIn->getLabel("dumprq")->makeChained("dump", undef, sub {
		if ($_[1]->getRow()->get("what") eq "packets") {
			$tPackets->dumpAll();
		}
	});

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		This function inherits the options from <pre>Triead::start()</pre> as usual and
		adds the option <quote>from</quote> of its own. This option's value is then used as
		the name of nexus to import for reading. The row types of the labels
		from that imported facet are then used to create the table and
		aggregation.
		</para>

		<para>
		The table and aggregation themselves are the same as in
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;,
		so I won't go into much detail describing them.  The only big change is
		the use of SimpleAggergator instead of a manually-built one.  The
		filter logic allows to delete the old raw data without propagation of
		the aggregation changes caused by it.
		</para>

		<para>
		Then the output nexus is created. The creation passes through all the
		incoming data, short-circuiting the input and output, and adds the
		extra label for the aggregated output. The call
		<pre>$faIn->getFnReturn()->getLabelHash()</pre> pulls all the labels
		and their names from the input facet, convenient for passing the
		data directly through to the output.  Just like an FnReturn,
		the Facet construction with <pre>makeNexus()</pre> has the option
		<quote>chainFront</quote> set to 1 by default, and thus when it chains
		the labels from the pass-through ones, they are chained on the front.
		This works very nicely: this way the input data passes through first
		and only then the input goes to the computational labels and produces
		the results that follow it into the output facet.
		</para>

		<para>
		The table dump is implemented after the output facet is defined because
		it needs the print label from that facet to send the results to.
		That print label ends up with two sources of datra for it. One is
		the eponymous label from the input facet, that passes the print
		requests from the previous stage of the pipeline. Another one is the
		table dump logic from this thread. Both are fine and can be mixed
		together.
		</para>

		<para>
		And after that it's all usual <pre>readyReady()</pre> and <pre>mainLoop()</pre>.
		</para>

		<para>
		The <pre>hourlyToDailyT()</pre> is very similar, so I won't even show it here, you can
		find the full text in the sources.
		</para>

	</sect1>
</chapter>
