<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5CR3//EN"
	"http://www.oasis-open.org/docbook/xml/4.5CR3/docbookx.dtd" [
<!ENTITY % userents SYSTEM "file:///ENTS/user.ent" >
%userents;
]>

<!--
(C) Copyright 2011-2013 Sergey A. Babkin.
This file is a part of Triceps.
See the file COPYRIGHT for the copyright notice and license information
-->

<chapter id="ch_mt" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Multithreading</title>

	<sect1 id="sc_mt_concepts">
		<title>Triceps multithreading concepts</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<para>
		When running the CEP models, naturally the threads have to be connected
		by the queues for the data exchange. The use of queues is extremely
		popular but also notoriously bug-prone.
		</para>

		<para>
		The idea of the multithreading support in Triceps is to make writing
		the multithreaded model easier. To make writing the good code easy and
		writing the bad code hard. But of course you don't have to use it, if 
		it feels too constraining, you can always make your own.
		</para>

		<para>
		The diagram in
		<xref linkend="fig_mt_overview" xrefstyle="select: label nopage"/>&xrsp;
		shows all the main elements of a multithread Triceps application.
		</para>

		<figure id="fig_mt_overview" >
			<title>Triceps multithreaded application.</title>
			<xi:include href="file:///FIGS/thread-010-over.xml"/> 
		</figure>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<indexterm>
			<primary>fragment</primary>
		</indexterm>
		<para>
		The Triceps application is embodied in the class App. It's possible to
		have multiple Apps in one program.
		</para>

		<para>
		Each thread has multiple parts to it. First, of course, there is the
		OS-level (or, technically, library-level, or Perl-level) thread where
		the code executes. And then there is a class that represents this
		thread and its place in the App. To reduce the naming conflict, this
		class is creatively named Triead (pronounced still <quote>thread</quote>). In the
		discussion I use the word <quote>thread</quote> for both concepts, the OS-level
		thread and the Triead, and it's usually clear from the context which
		one I mean. But sometimes it's particularly important to make the
		distinction, and then I name one or the other explicitly.
		</para>

		<para>
		The class Triead itself is largely opaque, allowing only a few methods
		for introspection. But there is a control interface to it, called
		TrieadOwner. The Triead is visible from the outside, the TrieadOwner
		object is visible only in the OS thread that owns the Triead. The
		TrieadOwner manages the thread state and acts as the intermediary in
		the thread's communications with the App.
		</para>

		<para>
		The data is passed between the threads through the Nexuses. A Nexus is
		unidirectional, with data going only one way, however it may have
		multiple writers and multiple readers. All the readers see the exact
		same data, with rowops going in the exact same order (well, there will
		be other policies in the future as well, but for now there is only one
		policy).
		</para>

		<para>
		A Nexus passes through the data for multiple labels, very much like an
		FnReturn does (and indeed there is a special connection between them).
		A Nexus also allows to export the row types and table types from one
		thread to another.
		</para>

		<para>
		A Nexus is created by one thread, and then the other threads connect to it.
		The thread that creates the Nexus determines what labels will it contain,
		and what row types and table types to export.
		</para>

		<para>
		A Nexus gets connected to the Trieads through the Facets (in the diagram,
		the Facets are shown as flat spots on the round Nexuses). A Facet is
		a connection point between the Nexus and the Triead. Each Facet is for
		either reading or writing. And there may be only one Facet between a
		given Nexus and a given Triead, you can't make multiple connections
		between them. As a consequence, a thread can't both write and read to
		the same Nexus, it can do only one thing. This might actually be an
		overly restrictive limitation and might change in the future but that's
		how things work now.
		</para>

		<indexterm>
			<primary>Nexus</primary>
			<secondary>reverse</secondary>
		</indexterm>
		<para>
		Each Nexus also has a direction: either direct (<quote>downwards</quote>) or reverse
		(<quote>upwards</quote>). How does it know, which direction is down and
		whih is up? It doesn't. You tell it by designating a Nexus one way or the other.
		And yes, the reverse Nexuses allow to build the models
		with loops. However the loops consisting of only the direct Nexuses are
		not allowed, nor of only reverse Nexuses. They would mess up the flow
		control. The proper loops must contain a mix of direct and reverse
		Nexuses.
		</para>

		<para>
		The direct Nexuses have a limited queue size and stop the writers when
		the queue fills up, until the data gets consumed, thus providing the
		flow control. The reverse Nexuses have an unlimited queue size, which
		allows to avoid the circular deadlocks. The reverse Nexuses also have
		a higher priority: if a thread is reading from a direct Nexus and a
		reverse one, with both having data available, it will read the data
		from the reverse Nexus first. This is to prevent the unlimited queues
		in the reverse Nexuses from the truly unlimited growth.
		</para>

		<para>
		Normally an App is built once and keeps running in this configuration
		until it stops. But there is a strong need to have the threads
		dynamically added and deleted too. For example, if the App running as a
		server, and clients connect to it, each client needs to have its
		thread(s) added when the client connects and then deleted when the client
		disconnects. This is handled through the concept of fragments. There is
		no Fragment class but when you create a Triead, you can specify a
		fragment name for it. Then it becomes possible to shut down and dispose
		the threads in a fragment after the fragment's work is done. 
		</para>
	</sect1>

	<sect1 id="sc_mt_triead_life">
		<title>The Triead lifecycle</title>

		<indexterm>
			<primary>Triead</primary>
			<secondary>stages</secondary>
		</indexterm>

		<para>
		Each Triead goes through a few stages in its life:
		</para>

		<itemizedlist>
			<listitem>
			declared
			</listitem>
			<listitem>
			defined
			</listitem>
			<listitem>
			constructed
			</listitem>
			<listitem>
			ready
			</listitem>
			<listitem>
			waited ready
			</listitem>
			<listitem>
			requested dead
			</listitem>
			<listitem>
			dead
			</listitem>
		</itemizedlist>

		<para>
		Note by the way that it's the stages of the Triead object. The OS-level
		thread as such doesn't know much about them, even though these stages
		do have some connections to its state.
		</para>

		<para>
		These stages always go in order and can not be skipped. However for
		convenience you can request a move directly to a further stage. This will just
		automatically pass through all the intermediate stages. Although, well,
		there is one exception: the <quote>waited ready</quote> and <quote>requested dead</quote> stages
		can get skipped on the way to <quote>dead</quote>. Other than that, there is always
		the sequence, so if you find out that a Triead is dead, you can be sure
		that it's also declared, defined, constructed and ready. The attempts
		to go to a previous stage are silently ignored.
		</para>

		<para>
		Now, what do these stages mean?
		</para>

		<variablelist>

			<varlistentry>
				<term>Declared:</term>
				<listitem>
				<para>
				The App knows the name of the thread and that this thread
				will eventually exist. When an App is asked to find the resources from
				this thread (such as Nexuses, and by the way, the Nexuses are
				associated with the threads that created them) it will know to wait
				until this thread becomes constructed, and then look for the resources.
				It closes an important race condition: the code that defines the Triead
				normally runs in a new OS thread but there is no way to tell when
				exactly will it run and do its work. If you had spawned a new thread and
				then attempted to get a nexus from it before it actually runs, the App
				would tell you that there is no such thread and fail. To get around it,
				you declare the thread first and then start it. Most of the time there
				is no need to declare explicitly, the library code that wraps the
				thread creation does it for you.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Defined:</term>
				<listitem>
				<para>
				The Triead object has been created and connected to the App.
				Since this is normally done from the new OS thread, it also implies
				that the thread is running and is busy about constructing the nexuses
				and whatever its own internal resources.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Constructed:</term>
				<listitem>
				<para>
				The Triead had constructed and exported all the nexuses
				that it planned to. This means that now these nexuses can be imported
				by the other threads (i.e. connected to the other threads). After this
				point the thread can not construct any more nexuses. However it can
				keep importing the nexuses from the other threads. It's actually a good
				idea to do all your exports, mark the thread constructed, and only then
				start importing. This order guarantees the absence of initialization deadlocks (which
				would be detected and will cause the App to be aborted). There are some
				special cases when you need to import a nexus from a thread that is not
				fully constructed yet, and it's possible, but requires more attention
				and a special override. I'll talk about it in more detail in 
				XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Ready:</term>
				<listitem>
				<para>
				The thread had imported all the nexuses it wanted and fully
				initialized all its internals (for example, if it needs to load data
				from a file, it might do that before telling that it's ready). After
				this point no more nexuses can be imported. A fine point is that the
				other threads may still be created, and they may do their exporting and
				importing, but once a thread is marked as ready, it's cast in bronze.
				And in the simple cases you don't need to worry about separating the
				constructed and ready stages, just initialize everything and mark the
				thread as ready.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Waited ready:</term>
				<listitem>
				<para>
				Before proceeding further, the thread has to wait for all
				the threads in App to be ready, or it would lose data when it tries to
				communicate with them. It's essentially a barrier. Normally both the
				stages <quote>ready</quote> and <quote>waited ready</quote> are advanced to with a single call
				<pre>readyReady()</pre>. With it the thread says <quote>I'm ready, and let me continue when
				everyone is ready</quote>. After that the actual work can begin. It's still
				possible to create more threads after that (normally, parts of the
				transient fragments), and until they all become ready, the App may
				temporarily become unready again, but that's a whole separate advanced
				topic that will be discussed in XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Requested dead:</term>
				<listitem>
				<para>
				This is the way to request a thread to exit. Normally
				some control thread will decide that the App needs to exit and will
				request all its threads to die. The threads will get these requests,
				perform their last rites and exit. The threads don't have to get this
				request to exit, they can also always decide to exit on their own. When
				a thread is requested to die, all the data communication with it stops.
				No more data will get to it through the nexuses and any data it sends
				will be discarded. It might churn a little bit through the data in its
				input buffers but any results produced will be discarded. The good
				practice is to make sure that all the data is drained before requesting
				a thread to die. Note that the nexuses created by this thread aren't
				affected at all, they keep working as usual. It's the data connections
				between this thread and any nexuses that get broken.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Dead:</term>
				<listitem>
				<para>
				The thread had completed its execution and exited. Normally you
				don't need to mark this explicitly. When the thread's main function
				returns, the library will do it for you. Marking the thread dead also
				drives the harvesting of the OS threads: the harvesting logic will
				perform a <pre>join()</pre> (not to be confused with SQL join) of the thread and
				thus free the OS resources. The dead Trieads are still visible in the
				App (except for some special cases with the fragments), and their
				nexuses continue working as usual (even including the special cases
				with the fragments), the other threads can keep communicating through
				them for as long as they want. 
				</para>
				</listitem>
			</varlistentry>
		</variablelist>
	</sect1>

	<sect1 id="sc_mt_pipeline">
		<title>Multithreaded pipeline</title>

		<indexterm>
			<primary>pipeline</primary>
		</indexterm>
		<para>
		The multithreaded models are well suited for running the pipelines,
		so that is going to be the first example of the threads. The full text of the example
		can be found in <pre>t/xTrafficAggMt.t</pre> in the class Traffic1.
		It's a variation of an already shown example, the traffic
		data aggregation from 
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;.
		The short recap is that it gets
		the data for each network packet going through and keeps it for some
		time, aggregates the data by the hour and keeps it for a longer time,
		and aggregates it by the day and keeps for a longer time yet. This
		multi-stage computation naturally matches the pipeline approach.
		</para>

		<para>
		Since this new example highlights different features than the original one,
		I've changed it logic a little: it updates both the
		hourly and daily summaries on every packet received. And I didn't
		bother to implement the part with the automatic cleaning of the old
		data, it doesn't add anything interesting to the pipeline works.
		</para>

		<para>
		The pipeline topologies are quite convenient for working with the
		threads. The parallel computations create a possibility of things
		happening in an unpredictable order and producing unpredictable
		results. The pipeline topology allows the parallelism and at the same
		time also keeps the data in the same predictable order, with no
		possibility of rows overtaking each other.
		</para>

		<para>
		The computation in this example is split into the following threads:
		</para>

		<itemizedlist>
			<listitem>
			Read the input, parse and send the data into the model.
			</listitem>

			<listitem>
			Store the recent data and aggregate it by the hour.
			</listitem>

			<listitem>
			Store the hourly data and aggregate it by the day.
			</listitem>

			<listitem>
			Store the daily data.
			</listitem>

			<listitem>
			Get the data at the end of the pipeline and print it.
			</listitem>
		</itemizedlist>

		<para>
		The result of each aggregation gets stored in a table in the next thread,
		which then uses the same table for the next stage of aggregation.
		</para>

		<para>
		Technically, each stage only needs the data from the previous
		stage, but to get the updates to the printing stage (since we want
		to print the original updates, daily and hourly), they all go all
		the way through.
		</para>

		<para>
		Dumping the contents of the tables also requires some special support.
		Each table is local to its thread and can't be accessed from the other
		threads. To dump its contents, the dump request needs to be sent to its
		thread, which would extract the data and send it through. There are
		multiple ways to deal with the dump results. One is to have a special
		label for each table's dump and propagate it to the last stage to
		print. If all that is needed is text, another way is to have one label that allows to send
		strings is good enough, all the dumps can send the data converted to
		text into it, and it would go all the way through the pipeline.
		For this example I've picked the last approach.
		</para>

		<para>
		And now is time to show some code. The main part goes like this:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
Triceps::Triead::startHere(
	app => "traffic",
	thread => "print",
	main => \&printT,
);
</pre>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<para>
		The <pre>startHere()</pre> creates an App and starts a Triead in the current OS
		thread. <quote>Here</quote> in the method name stands for <quote>in the current OS thread</quote>.
		<quote>traffic</quote> is the app name, <quote>print</quote> the thread name. This thread
		will be the end of the pipeline, and it will create the rest of the
		threads. This is a convenient pattern when the results of the model
		need to be fed back to the current thread, and it works out very
		conveniently for the unit tests. <pre>printT()</pre> is the body function of
		this printing thread:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub printT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	Triceps::Triead::start(
		app => $opts->{app},
		thread => "read",
		main => \&readerT,
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "raw_hour",
		main => \&rawToHourlyT,
		from => "read/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "hour_day",
		main => \&hourlyToDailyT,
		from => "raw_hour/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "day",
		main => \&storeDailyT,
		from => "hour_day/data",
	);

	my $faIn = $owner->importNexus(
		from => "day/data",
		as => "input",
		import => "reader",
	);

	$faIn->getLabel("print")->makeChained("print", undef, sub {
		&send($_[1]->getRow()->get("text"));
	});
	for my $tag ("packet", "hourly", "daily") {
		makePrintLabel($tag, $faIn->getLabel($tag));
	}

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		<pre>startHere()</pre> accepts a number of fixed options plus arbitrary options
		that it doesn't care about by itself but passes through to the thread's main
		function, which are then the responsibility of the main function to
		parse. To reiterate, the main function gets all the options from the
		call of <pre>startHere()</pre>, both these that <pre>startHere()</pre> parses and these that
		it simply passes through. <pre>startHere()</pre> also adds one more option on its
		own: <quote>owner</quote> containing the TrieadOwner object that the thread uses to
		communicate with the rest of the App.
		</para>

		<para>
		In this case <pre>printT()</pre> doesn't have any extra options on its own,
		it's just happy to get <pre>startHere()</pre>'s standard set that it takes all
		together from <pre>@Triceps::Triead::opts</pre>.
		</para>

		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Unit</primary>
		</indexterm>
		<indexterm>
			<primary>UnitClearingTrigger</primary>
		</indexterm>
		<para>
		It gets the TrieadOwner object <pre>$owner</pre> from the option appended by
		<pre>startHere()</pre>. Each TrieadOwner is created with its own Unit, so the unit
		is obtained from it to create the thread's model in it. Incidentally,
		the TrieadOwner  also acts as a clearing trigger object for the Unit,
		so when the TrieadOwner is destroyed, it properly clears the Unit.
		</para>

		<para>
		Then it goes and creates all the threads of the pipeline. The <pre>start()</pre>
		works very much like <pre>startHere()</pre>, only it actually creates a new thread
		and starts the main function in it. The main function can be the same
		whether it runs through <pre>start()</pre> or <pre>startHere()</pre>. The special catch is
		that the options to <pre>start()</pre> must contain only the plain Perl values,
		not Triceps objects. It has to do with how Perl works with threads: it
		makes a copy of every value for the new thread, and it cant's copy the
		XS objects, so they simply become undefined in the new thread.
		</para>

		<para>
		All but the first thread in the pipeline have the extra option <quote>from</quote>: it
		specifies the input nexus for this thread, and each thread creates an
		output nexus <quote>data</quote>. A nexus it named relatively to the
		thread that created it, so when the option <quote>from</quote> says <quote>day/data</quote>, it's
		the nexus <quote>data</quote> created by the thread <quote>day</quote>.
		</para>

		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<para>
		So, the pipeline gets all connected sequentially until eventually
		<pre>printT()</pre> imports the nexus at its tail. <pre>importNexus()</pre> returns a
		Facet, which is the thread's API to the nexus. A facet looks very much
		like an FnReturn for most purposes, with a few additions. It even has a
		real FnReturn in it, and you work with the labels of that FnReturn to get the
		data out of the nexus (or to send data into the nexus). You could potentially
		use an FnBinding with that FnReturn but the typical pattern for reading
		from a facet is different: just get its labels 
		and chain the handling labels directly to them.
		</para>

		<para>
		The option <quote>as</quote> of <pre>importNexus()</pre> gives the name to the facet and to its
		same-named FnReturn (without it the facet would be named the same as
		the short name of the nexus, in this case <quote>data</quote>). The option <quote>import</quote>
		tells whether this thread will be reading or writing the nexus, and
		in this case it's reading.
		</para>

		<para>
		By the time the pipeline gets to the last stage, it has a few
		labels in its facet:
		</para>

		<itemizedlist>
			<listitem>
			<pre>print</pre> - carries the direct text lines to print in its field <pre>text</pre>,
			and its contents gets printed.
			</listitem>

			<listitem>
			<pre>dumprq</pre> - carries the dump requests to the tables, and the printing
			thread doesn't care about it.
			</listitem>

			<listitem>
			<pre>packet</pre> - carries the raw data about the packets.
			</listitem>

			<listitem>
			<pre>hourly</pre> - carries the hourly summaries.
			</listitem>

			<listitem>
			<pre>daily</pre> - carries the daily summaries.
			</listitem>
		</itemizedlist>

		<para>
		The last three get also printed but this time as whole rows.
		</para>

		<para>
		And after everything is connected, the thread both tells that it's
		ready and waits for all the other threads to become ready by calling
		<pre>readyReady()</pre>. Then its the run time, and <pre>mainLoop()</pre> takes care of it:
		it keeps reading data from the nexus and
		processes it until it's told to shutdown.
		The shutdown will be controlled by the file reading
		thread at the start of the pipeline. The processing is done by getting
		the rowops from the nexus and calling them on the appropriate label in
		the facet, which then calls the the labels chained from it, and that
		gets all the rest of the thread's model running.
		</para>

		<para>
		The reader thread drives the pipeline:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub readerT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	my $rtPacket = Triceps::RowType->new(
		time => "int64", # packet's timestamp, microseconds
		local_ip => "string", # string to make easier to read
		remote_ip => "string", # string to make easier to read
		local_port => "int32", 
		remote_port => "int32",
		bytes => "int32", # size of the packet
	);

	my $rtPrint = Triceps::RowType->new(
		text => "string", # the text to print (including \n)
	);

	my $rtDumprq = Triceps::RowType->new(
		what => "string", # identifies, what to dump
	);

	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			packet => $rtPacket,
			print => $rtPrint,
			dumprq => $rtDumprq,
		],
		import => "writer",
	);

	my $lbPacket = $faOut->getLabel("packet");
	my $lbPrint = $faOut->getLabel("print");
	my $lbDumprq = $faOut->getLabel("dumprq");

	$owner->readyReady();

	while(&readLine) {
		chomp;
		# print the input line, as a debugging exercise
		$unit->makeArrayCall($lbPrint, "OP_INSERT", "> $_\n");

		my @data = split(/,/); # starts with a command, then string opcode
		my $type = shift @data;
		if ($type eq "new") {
			$unit->makeArrayCall($lbPacket, @data);
		} elsif ($type eq "dump") {
			$unit->makeArrayCall($lbDumprq, "OP_INSERT", $data[0]);
		} else {
			$unit->makeArrayCall($lbPrint, "OP_INSERT", "Unknown command '$type'\n");
		}
		$owner->flushWriters();
	}

	{
		# drain the pipeline before shutting down
		my $ad = Triceps::AutoDrain::makeShared($owner);
		$owner->app()->shutdown();
	}
}
</pre>

		<para>
		It starts by creating the nexus with the initial set of the labels: for
		the data about the network packets, for the lines to be printed at the
		end of the pipeline and for the dump requests to the tables in the
		other threads. It gets exported for the other threads to import, and
		also imported right back into this thread, for writing. And then the
		setup is done, <pre>readyReady()</pre> is called, and the processing starts.
		</para>

		<para>
		It reads the CSV lines, splits them, makes a decision if it's a data
		line or dump request, and one way or the other sends it into the nexus.
		The data sent to a facet doesn't get immediately forwarded to the
		nexus. It's collected internally in a tray, and then <pre>flushWriters()</pre>
		sends it on. The <pre>mainLoop()</pre> shown in <pre>printT</pre> calls <pre>flushWriters()</pre>
		automatically after every tray it processes from the input. But when
		reading from a file you've got to do it yourself. Of course, it's more
		efficient to send through multiple rows at once, so a smarter
		implementation would check if multiple lines are available from the
		file and send them in larger bundles.
		</para>

		<indexterm>
			<primary>shutdown</primary>
		</indexterm>
		<indexterm>
			<primary>drain</primary>
		</indexterm>
		<para>
		The last part is the shutdown. After the end of file is reached, it's
		time to shut down the application. You can't just shut down it right
		away because there still might be data in the pipeline, and if you shut
		it down, that data will be lost. The right way is to drain the pipeline
		first, and then do the shutdown when the app is drained.
		<pre>AutoDrain::makeShared()</pre> creates a scoped drain: the drain request for
		all the threads is started when this object is created, and the object
		construction completes when the drain succeeds. When the object is
		destroyed, that releases the drain. So in this case the drain succeeds and
		then the app gets shut down.
		</para>

		<para>
		The shutdown causes the <pre>mainLoop()</pre> calls in all the other threads to
		return, and the threads to exit. Then <pre>startHere()</pre> in the first thread
		has the special logic in it that joins all the started threads after
		its own main function returns and before it completes. After that the
		script continues on its way and is free to exit. 
		</para>

		<para>
		The rest of this example might be easier to understand by looking at an
		example of a run first. The lines in bold are the copies of
		the input lines that <pre>readerT()</pre> reads from the input and
		sends into the pipeline, and <pre>printT()</pre> faithfully
		prints.
		</para>

		<para>
		<pre>input.packet</pre> are the rows that reach the <pre>printT</pre> on the <pre>packet</pre> label
		(remember, <quote>input</quote> is the name with which it imports its input nexus).
		<pre>input.hourly</pre> is the data aggregated by the hour intervals (and also by
		the IP addresses, dropping the port information), and <pre>input.daily</pre>
		further aggregates it per day (and again per the IP addresses). The
		timestamps in the hourly and daily rows are truncated to the start
		of the hour or day.
		</para>

		<para>
		And the lines without any prefixes are the dumps of the table contents
		that again reach the <pre>printT()</pre> through the <quote>print</quote> label:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<exdump>
> new,OP_INSERT,1330886011000000,1.2.3.4,5.6.7.8,2000,80,100
input.packet OP_INSERT time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
> new,OP_INSERT,1330886012000000,1.2.3.4,5.6.7.8,2000,80,50
input.packet OP_INSERT time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
input.hourly OP_DELETE time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
> new,OP_INSERT,1330889612000000,1.2.3.4,5.6.7.8,2000,80,150
input.packet OP_INSERT time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
> new,OP_INSERT,1330889811000000,1.2.3.4,5.6.7.8,2000,80,300
input.packet OP_INSERT time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
input.hourly OP_DELETE time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
> new,OP_INSERT,1330972411000000,1.2.3.5,5.6.7.9,3000,80,200
input.packet OP_INSERT time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
input.hourly OP_INSERT time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
input.daily OP_INSERT time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> new,OP_INSERT,1331058811000000
input.packet OP_INSERT time="1331058811000000" 
> new,OP_INSERT,1331145211000000
input.packet OP_INSERT time="1331145211000000" 
> dump,packets
time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
> dump,hourly
time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> dump,daily
time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
</exdump>

		<para>
		Note that the order of the lines is completely nice and predictable,
		nothing goes out of order. Each nexus preserves the order of the rows
		put into it, and the fact that there is only one writer per nexus and
		that every thread is fed from only one nexus, avoids the races.
		</para>

		<para>
		Let's look at the thread that performs the aggregation by the hour:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 edited -->
<pre>
# compute an hour-rounded timestamp (in microseconds)
sub hourStamp # (time)
{
	return $_[0]  - ($_[0] % (1000*1000*3600));
}

sub rawToHourlyT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {
		@Triceps::Triead::opts,
		from => [ undef, \&Triceps::Opt::ck_mandatory ],
	}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	# The current hour stamp that keeps being updated;
	# any aggregated data will be propagated when it is in the
	# current hour (to avoid the propagation of the aggregator clearing).
	my $currentHour;

	my $faIn = $owner->importNexus(
		from => $opts->{from},
		as => "input",
		import => "reader",
	);

	# the full stats for the recent time
	my $ttPackets = Triceps::TableType->new($faIn->getLabel("packet")->getRowType())
		->addSubIndex("byHour", 
			Triceps::IndexType->newPerlSorted("byHour", undef, sub {
				return &hourStamp($_[0]->get("time")) <=> &hourStamp($_[1]->get("time"));
			})
			->addSubIndex("byIP", 
				Triceps::IndexType->newHashed(key => [ "local_ip", "remote_ip" ])
				->addSubIndex("group",
					Triceps::IndexType->newFifo()
				)
			)
		)
	;

	# type for a periodic summary, used for hourly, daily etc. updates
	my $rtSummary;

	Triceps::SimpleAggregator::make(
		tabType => $ttPackets,
		name => "hourly",
		idxPath => [ "byHour", "byIP", "group" ],
		result => [
			# time period's (here hour's) start timestamp, microseconds
			time => "int64", "last", sub {&hourStamp($_[0]->get("time"));},
			local_ip => "string", "last", sub {$_[0]->get("local_ip");},
			remote_ip => "string", "last", sub {$_[0]->get("remote_ip");},
			# bytes sent in a time period, here an hour
			bytes => "int64", "sum", sub {$_[0]->get("bytes");},
		],
		saveRowTypeTo => \$rtSummary,
	);

	$ttPackets->initialize();
	my $tPackets = $unit->makeTable($ttPackets, "tPackets");

	# Filter the aggregator output to match the current hour.
	my $lbHourlyFiltered = $unit->makeDummyLabel($rtSummary, "hourlyFiltered");
	$tPackets->getAggregatorLabel("hourly")->makeChained("hourlyFilter", undef, sub {
		if ($_[1]->getRow()->get("time") == $currentHour) {
			$unit->call($lbHourlyFiltered->adopt($_[1]));
		}
	});

	# update the notion of the current hour before the table
	$faIn->getLabel("packet")->makeChained("processPackets", undef, sub {
		my $row = $_[1]->getRow();
		$currentHour = &hourStamp($row->get("time"));
		# skip the timestamp updates without data
		if (defined $row->get("bytes")) {
			$unit->call($tPackets->getInputLabel()->adopt($_[1]));
		}
	});

	# The makeNexus default option chainFront => 1 will make
	# sure that the pass-through data propagates first, before the
	# processed data.
	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			$faIn->getFnReturn()->getLabelHash(),
			hourly => $lbHourlyFiltered,
		],
		import => "writer",
	);

	my $lbPrint = $faOut->getLabel("print");

	# the dump request processing
	$tPackets->getDumpLabel()->makeChained("printDump", undef, sub {
		$unit->makeArrayCall($lbPrint, "OP_INSERT", $_[1]->getRow()->printP() . "\n");
	});
	$faIn->getLabel("dumprq")->makeChained("dump", undef, sub {
		if ($_[1]->getRow()->get("what") eq "packets") {
			$tPackets->dumpAll();
		}
	});

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		This function inherits the options from <pre>Triead::start()</pre> as usual and
		adds the option <quote>from</quote> of its own. This option's value is then used as
		the name of nexus to import for reading. The row types of the labels
		from that imported facet are then used to create the table and
		aggregation.
		</para>

		<para>
		The table and aggregation themselves are the same as in
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;,
		so I won't go into much detail describing them.  The only big change is
		the use of SimpleAggergator instead of a manually-built one.  The
		filter logic allows to delete the old raw data without propagation of
		the aggregation changes caused by it.
		</para>

		<para>
		Then the output nexus is created. The creation passes through all the
		incoming data, short-circuiting the input and output, and adds the
		extra label for the aggregated output. The call
		<pre>$faIn->getFnReturn()->getLabelHash()</pre> pulls all the labels
		and their names from the input facet, convenient for passing the
		data directly through to the output.  Just like an FnReturn,
		the Facet construction with <pre>makeNexus()</pre> has the option
		<quote>chainFront</quote> set to 1 by default, and thus when it chains
		the labels from the pass-through ones, they are chained on the front.
		This works very nicely: this way the input data passes through first
		and only then the input goes to the computational labels and produces
		the results that follow it into the output facet.
		</para>

		<para>
		The table dump is implemented after the output facet is defined because
		it needs the print label from that facet to send the results to.
		That print label ends up with two sources of datra for it. One is
		the eponymous label from the input facet, that passes the print
		requests from the previous stage of the pipeline. Another one is the
		table dump logic from this thread. Both are fine and can be mixed
		together.
		</para>

		<para>
		And after that it's all usual <pre>readyReady()</pre> and <pre>mainLoop()</pre>.
		</para>

		<para>
		The <pre>hourlyToDailyT()</pre> is very similar, so I won't even show it here, you can
		find the full text in the sources.
		</para>
	</sect1>

	<sect1 id="sc_mt_objects">
		<title>Object passing between threads</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<indexterm>
			<primary>XS</primary>
		</indexterm>
		<para>
		A limitation of the Perl threads is that no variables can be shared
		between them. Well, there are the special shared variables but
		they're are very special and have great many limitations on their own.
		When a new thread gets created, it gets a copy of all the
		variables of the parent. That is, of all the plain Perl variables. With
		the XS extensions your luck may vary: the variables might get copied,
		might become undefined, or just become broken (if the XS module is not
		threads-aware). Copying the XS variables requires a quite high overhead
		at all the other times, so Triceps doesn't do it and all the Triceps
		object become undefined in the new thread.
		</para>

		<para>
		This model of behavior for a package is marked by creating the
		method <pre>CLONE_SKIP</pre> in it:
		</para>

<pre>
sub CLONE_SKIP { 1; }
</pre>

		<para>
		All the Triceps packages define it, and it's the best practice to
		define it in your packages as well.
		</para>

		<para>
		However the threads are useless without communication, and
		Triceps provides a way to pass around certain objects through the
		Nexuses.
		</para>

		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<indexterm>
			<primary>Rowop</primary>
		</indexterm>
		<para>
		First, obviously, the Nexuses are intended to pass through the Rowops.
		These Rowops coming out of a nexus are not the same Rowop objects that
		went in. Rowop is a single-threaded object and can not be shared by two
		threads. Instead it gets converted to an internal form while in the
		nexus, and gets re-created when it comes out, pointing to the same Row object and to the
		correct Label in the local Facet.
		</para>

		<para>
		Then, again obviously, the Facets get imported to the other threads
		as an interface of the Nexus, together with their row types.
		</para>

		<indexterm>
			<primary>RowType</primary>
		</indexterm>
		<indexterm>
			<primary>TableType</primary>
		</indexterm>
		<para>
		And two more types of objects can be exported through a Nexus: the
		RowTypes and TableTypes. They get exported through the options as in
		this example:
		</para>

<pre>
$fa = $owner->makeNexus(
    name => "nx1",
    labels => [
        one => $rt1,
        two => $lb,
    ], 
    rowTypes => [
        one => $rt2,
        two => $rt1,
    ], 
    tableTypes => [
        one => $tt1,
        two => $tt2,
    ], 
    import => "writer",
); 
</pre>

		<para>
		As you can see, the namespaces for the labels, row types and table
		types are completely independent, and the same names can be reused in
		each of them for different meaning. All the three sections are
		optional, so if you want, you can export only the types in the nexus,
		without any labels.
		</para>

		<para>
		They can then be extracted from the imported facet as:
		</para>

<pre>
$rt1 = $fa->impRowType("one");
$tt1 = $fa->impTableType("one");
</pre>

		<para>
		Or the whole set of name-value pairs can be obtained with:
		</para>

<pre>
@rtset = $fa->impRowTypesHash();
@ttset = $fa->impTableTypesHash();
</pre>

		<para>
		The exact table types and row types (by themselves or in the table
		types or labels) in the importing thread will be copied. It's
		technically possible to share the references to the same row type
		from multiple threads in
		the &Cpp; code but it's more efficient to make a separate copy for each
		thread, and thus the Perl API goes along the more efficient way.
		</para>

		<para>
		The import is smart in the sense that it preserves the sameness of the
		row types: if in the exporting thread the same row type was referred
		from multiple places in the <pre>labels</pre>, <pre>rowTypes</pre> and <pre>tableTypes</pre> sections,
		in the imported facet that would again be the same row type object (even
		though of course not the one that has been exported but its copy). This
		again helps with the efficiency when various objects decide if the rows
		created by this and that type are matching.
		</para>

		<indexterm>
			<primary>TableType</primary>
		</indexterm>
		<para>
		This is all well until you want to export a table type that has an
		index with a Perl sort condition in it, or an aggregator with the Perl
		code. The Perl code objects are tricky: they get copied OK when a new
		thread is created but the attempts to import them through a nexus later
		cause a terrible memory corruption. So Triceps doesn't allow to export
		the table types with the function references in them. 
		But it provides an
		alternative solution: the code snippets can be specified as the source
		code, as described in
		<xref linkend="sc_code" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;. 
		They get compiled when the table type gets initialized. When a
		table type gets imported through a nexus, it brings the source code
		with it. The imported table types are always uninitialized, so at
		initialization time the source code gets compiled in the new thread and
		works.
		</para>

		<para>
		It all works transparently: just specify a string instead of a function
		reference when creating the index, and it will be recognized and
		processed. For example:
		</para>

<pre>
$it= Triceps::IndexType->newPerlSorted("b_c", undef, '
    my $res = ($_[0]->get("b") <=> $_[1]->get("b")
        || $_[0]->get("c") <=> $_[1]->get("c"));
    return $res;
    '
);
</pre>

		<para>
		Before the code gets compiled, it gets wrapped into a <pre>sub { ... }</pre>, so
		don't write your own <pre>sub</pre> in the code string, that would be an error.
		</para>

		<para>
		To recap the differences between the code references and the
		source code snippets format:
		</para>

		<para>
		When you compile a function, it carries with it the lexical context. So
		you can make the closures that refer to the <quote>my</quote> variables in their
		lexical scope. With the source code you can't do this. The table type
		compiles them at initialization time in the context of the main
		package, and that's all they can see. Remember also that the global
		variables are not shared between the threads, so if you refer to a
		global variable in the code snippet and rely on a value in that
		variable, it won't be present in the other threads (unless the other
		threads are direct descendants and the value was set before their
		creation).
		</para>

		<para>
		There is also the issue of arguments that can be specified for these
		functions. Triceps is smart enough to handle the arguments that are
		one of:
		</para>

		<itemizedlist>
			<listitem>
			<pre>undef</pre>
			</listitem>
			<listitem>
			integer
			</listitem>
			<listitem>
			floating-point
			</listitem>
			<listitem>
			string
			</listitem>
			<listitem>
			Triceps::RowType object
			</listitem>
			<listitem>
			Triceps::Row object
			</listitem>
			<listitem>
			reference to an array or hash thereof
			</listitem>
		</itemizedlist>

		<para>
		It converts the data to an internal &Cpp; representation in the nexus and
		then converts it back on import. So, if a TableType has all the code in
		it in the source form, and the arguments for this code within the
		limits of this format, it can be exported through the nexus. Otherwise
		an attempt to export it will fail.
		</para>

		<indexterm>
			<primary>SimpleOrderedIndex</primary>
		</indexterm>
		<indexterm>
			<primary>aggregation</primary>
		</indexterm>
		<para>
		The SimpleOrderedIndex uses the source code format for the
		functions it generates, so they will pass through the nexuses.
		And if you specify the aggregator functions as code snippets, you
		can export the table types with them through the nexuses too.
		</para>

		<indexterm>
			<primary>SimpleAggregator</primary>
		</indexterm>
		<para>
		However things didn't work out so well for the SimpleAggregator.
		I've found that I can't just do it within the current aggregation
		infrastructure. As mentioned in
		<xref linkend="sc_aggregation_optimized" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;,
		the aggregators don't have the same kind of initialization function
		as indexes (one that would run at the table type initialization
		time), and that becomes the deal-breaker. 
		</para>

		<para>
		Fortunately, some thinking had showed that this feature is not really
		needed. There usually just isn't any need to export a table type with
		aggregators. 
		So it's a nice feature to have overall but not urgent.
		Moreover, there is a need to export the table types with
		many elements stripped. 
		</para>

		<para>
		What is to be stripped and why?
		The most central part of the table type is its primary index. It
		defines how the data gets organized. And then the secondary indexes and
		aggregators perform the computations from the data in the table. The
		tables can not be shared between threads, and thus the way to copy a
		table between the threads is to export the table type and send the
		data, then let the other thread construct a copy of the table from that.
		But the table created in another thread really needs only the base data
		organization. If it does any computations on that data, that would be
		its own computations, different than the ones in the exporting thread.
		So all it needs to get is the basic table type with the primary index,
		very rarely some secondary indexes, and pretty much never the
		aggregators. The importing thread would then add its own secondary
		indexes and aggregators before initializing its table type and
		constructing the table from it.
		</para>

		<para>
		The way to get such a stripped table type with only the fundamentally
		important parts is:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental();
</pre>

		<para>
		That copies the row type and the primary index (the whole path to the
		first leaf index type) and leaves alone the rest. All the aggregators
		on all the indexes, even on the primary one, are not included in the
		copy. In the context of the full nexus, making it can look like:
		</para>

<pre>
$facet = $owner->makeNexus(
    name => "data"
    labels => [ @labels ],
    tableTypes => [
         mytable => $mytable->getType()->copyFundamental(),
    ],
    import => "writer",
);
</pre>

		<para>
		In case if more index types need to be included, they can be specified
		by path in the arguments of <pre>copyFundamental()</pre>:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
    [ "byDate", "byAddress", "fifo" ],
    [ "byDate", "byPriority", "fifo" ],
);
</pre>

		<para>
		The paths may overlap, as shown here, and the matching subtrees will be
		copied correctly, still properly overlapping in the result. There is
		also a special syntax:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
    [ "secondary", "+" ],
);
</pre>

		<para>
		The <pre>"+"</pre> in the path means <quote>do the path to the first leaf index of that
		subtree</quote> and saves the necessity to write out the whole path.
		</para>

		<para>
		Finally, what if you don't want to include the original primary index
		at all? You can use the string <pre>"NO_FIRST_LEAF"</pre> as the first argument.
		That would skip it. You can still include it by using its explicit
		path, possibly at the other position.
		</para>

		<para>
		For example, suppose that you have a table type with two top-level
		indexes, <quote>first</quote> is the primary index and <quote>second</quote> as secondary, and
		make a copy:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
     "NO_FIRST_LEAF",
    [ "second", "+" ],
    [ "first", "+" ],
);
</pre>

		<para>
		In the copied table type the index <quote>second</quote> becomes primary and <quote>first</quote>
		secondary.
		</para>
	</sect1>

	<sect1 id="sc_mt_files">
		<title>Threads and file descriptors</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<indexterm>
			<primary>socket</primary>
		</indexterm>
		<indexterm>
			<primary>file</primary>
		</indexterm>
		<para>
		The interaction of the file descriptors (including sockets) with threads
		tends to be a somewhat thorny issue. The problems lie around the problem
		of geting a thread to  stop if it's stuck reading from a file descriptor.
		Triceps aims to be an one-stop shop for the threading solutions, so
		among other things it covers the interaction with the file descriptors.
		</para>

		<indexterm>
			<primary>revocation</primary>
		</indexterm>
		<para>
		The overall approach is that whenever a thread opens a file, it should register
		that file with its TrieadOwner object. Then when the thread is requested
		to die, that file will be revoked, waking up the thread if it's waiting
		for that file descriptor. This is an operation that requires finesse,
		with multiple possibilities for the race conditions, but Triceps
		covers all the complexity and takes care of everything.
		</para>

		<para>
		The registration of files with TrieadOwner is done with:
		</para>

<pre>
$to->track(*FILE);
$to->track($socket);
</pre>

		<para>
		The form of the argument differs depending on whether it's a plain
		Perl file handle or if it's a file object, such as the one returned
		by the socket creation methods. The plain perl file handles have to be specified
		in a glob form, with <pre>*</pre>.
		</para>

		<para>
		To unregister the file and close it, use:
		</para>

<pre>
$to->close(*FILE);
$to->close($socket);
</pre>

		<para>
		If you just want to unregister a file without closing it (not sure
		why would you want to do that, but why not), there is also a way:
		</para>

<pre>
$to->forget(*FILE);
$to->forget($socket);
</pre>

		<para>
		And there also are the methods that do the registration with the
		plain file descriptors:
		</para>

<pre>
$to->trackFd($fd);
$to->forgetFd($fd);
</pre>

		<para>
		The file descriptor methods underlie the file handle methods,
		a <pre>$to->trackFd(fileno(FILE));</pre> is really an equivalent
		of <pre>$to->track(*FILE);</pre>.
		</para>

		<para>
		But wait, there is more. It's annoying to close the files manually,
		and easy to miss too (especially if the code might die within an
		<pre>eval</pre>, with the file close sandwiched between them). 
		The scope-based file closing is much easier and more reliable: when
		you leave the scope, the file is guaranteed to get closed.
		Triceps provides the scope-based file handle management.
		Actuallly, if you use files-as-objects in Perl (in the form like
		<pre>$socket</pre>), Perl already does the scope-based management,
		closing the file when the last reference to it disappears.
		But Triceps adds the automatic forgetting of the file in the
		TrieadOwner as well.  And it's very, very important to unregister
		the file descriptors before closing them, or the file descriptor
		corruption will result when the thread exits.
		A scope-based file is registered like this:
		</para>

<pre>
$tf = $to->makeTrackedFile($file);
</pre>

		<indexterm>
			<primary>TrackedFile</primary>
		</indexterm>
		<para>
		<pre>$tf</pre> will contain a TrackedFile object that will control
		the life of the file tracking. <pre>$tf</pre> contains its own reference
		to the file handle, so until <pre>$tf</pre> is destroyed, the file
		handle will not be destroyed and thus will not be automatically closed.
		When <pre>$tf</pre> goes out of scope and gets
		destroyed, it will unregister the file from TrieadOwner and then
		discard its reference to the file handle, letting Perl close it if all
		the references are gone. 
		To find the file handle from the TrackedFile, use:
		</para>

<pre>
$file = $tf->get();
</pre>

		<para>
		It's also possible to close the file explicitly before <pre>$tf</pre> goes
		out of scope:
		</para>

<pre>
$tf->close();
</pre>

		<para>
		That will unregister the file and close it. And in this case close really
		means close: even if there are other references to the file handle, it will
		still get closed and could not be used through these other references any more
		either. After that, the final destruction of the TrackedFile object will have
		nothing to do.
		</para>

		<indexterm>
			<primary>FileInterrupt</primary>
		</indexterm>
		<para>
		In the &Cpp; API the file registration happens a bit differently, through a
		FileInterrupt object that keeps track of a set of file descriptors. Each
		trieadOwner object has a public field <pre>fileInterrupt_</pre> of that
		class. There is no scoped unregistration in the &Cpp; API yet, it should
		probably be added in the future. The Perl API actually also uses the
		FileInterrupt but with the high-level logic on top of it.
		</para>

		<indexterm>
			<primary>dup2</primary>
		</indexterm>
		<indexterm>
			<primary>SIGUSR2</primary>
		</indexterm>
		<para>
		In case if you wonder what exactly happens with the file handle during the
		revocation, let me tell you. Overall it follows the approach described
		in my book <biblioref linkend="Babkin10"/>,
		with the system call <pre>dup2()</pre> copying a
		descriptor of <pre>/dev/null</pre> opened read-only over the target file descriptor.
		Perl's file handle keeps owning that file descriptor id but now
		with a different contents in it. However there is a problem with
		the plain <pre>dup2()</pre>, it doesn't always interrupt the ongoing system call.  
		I've thought that on Linux it
		works reliably but then I've found that it works on the sockets but not
		on the pipes, and even with sockets the <pre>accept()</pre> seems to ignore it.
		So I've found a better solution: use a <pre>dup2()</pre> but then also
		send a signal (Triceps uses <pre>SIGUSR2</pre>) to the target thread, which
		has a dummy handler of that signal to avoid killing the process.
		Even if <pre>dup2()</pre> gets ignored by the
		current system call, the signal will get through and either make the ongoing system call return
		<pre>EINTR</pre> to make the user code retry or cause a system call restart in the
		OS. In either case the new file descriptor copied by <pre>dup2()</pre> will be
		discovered on the next attempt and cause the desired interruption. And
		unlike the signal used by itself, <pre>dup2()</pre> closes the race window around
		the signal.
		</para>

		<para>
		And another detour into the gritty details, what if the thread gets
		requested to die after the socket is opened but before it is tracked? 
		The answer is that the tracking enrollment will check whether
		the death request already happened, and if so, it will revoke the socket
		right away, before returning. So the reading loop will find the socket
		revoked right on its first iteration. It's one of these potential
		race conditions that Triceps prevents.
		</para>

		<para>
		Now returning back to the high-level Perl API.
		As it turns out, Perl doesn't allow to pass the file descriptors between the
		threads. Well, you sort of can pass them as arguments to another thread
		but then it ends up printing the error messages like these and
		corrupting the reference counts:
		</para>

<pre>
Unbalanced string table refcount: (1) for "GEN1" during global destruction.
Unbalanced string table refcount: (1) for "/usr/lib/perl5/5.10.0/Symbol.pm" during global destruction.
Scalars leaked: 1
</pre>

		<indexterm>
			<primary>socket</primary>
		</indexterm>
		<para>
		If you try to pass a file descriptor through trheads::shared, it
		honestly won't allow you, while the thread arguments pretend that they
		can and then fail.
		</para>

		<para>
		And it's something that is really needed, for more than one reason.
		If you write a TCP server, you typically have one thread accepting
		connections and then creating a new thread to handle each accepted
		socket. And the typical way to avoid polling on a socket is to have
		two threads handle it, one doing all the reading, another one
		doing all the writing. None of that works with Perl out of the box.
		Triceps comes to the rescue again, it gets done as follows:
		</para>

<pre>
# in one thread
$to->app()->storeCloseFile($name, $socket);

#----------

# in another thread
my ($tf, $socket) = $to->trackGetFile($name, 'r+');
</pre>

		<para>
		The first thing that needs to be clarified here is that even though
		the variables <pre>$to</pre> are named the same in both threads, they
		contain different TrieadOwner objects, each one belonging to its own thread.
		<pre>storeCloseFile()</pre> stores a copy of the socket in the App object and
		then closes it in the local thread (the copy stays open). <pre>trackGetFile()</pre> pulls the
		socket out of the App, registers it with the TrieadOwner and creates
		a TrackedFile scoped object for it, returning both the TrackedFile and
		the socket handle object. The file opening mode still has to be specified
		as a <pre>trackGetFile()</pre> argument because it can't be easily
		pulled out of the original file handle, and also because the other
		thread might want to use only a subset of the modes from the original.
		The mode can be specified in either of the fashions: 
		as <pre>r/w/a/r+/w+/a+</pre> or <pre></>/>>/+</+>/+>></pre>.
		</para>

		<para>
		The <pre>$name</pre> is a unique name by which
		this socket is known in the App, it has to be generated in some way
		to guarantee the uniqueness. But then the name is a plain string that
		can be passed between the threads, either as an argument at the thread
		creation time or in a Triceps Rowop going through a nexus.
		The whole procedure is easy, straightforward, and difficult to get wrong.
		</para>

		<para>
		Let's look at more variations of the same. What if you want to pass
		a file handle to more than one thread? It's a typical case when a TCP
		server accepts a connection and wants to start the separate reader
		and writer threads on that socket.  One way is to simply pass it twice,
		with different names. For example:
		</para>

<pre>
# in the acceptor thread
$to->app()->storeFile('name_r', $socket);
$to->app()->storeCloseFile('name_w', $socket);

#----------

# in the reader thread
my ($tf, $socket) = $to->trackGetFile('name_r', 'r');

#----------

# in the writer thread
my ($tf, $socket) = $to->trackGetFile('name_w', 'w');
</pre>

		<para>
		The <pre>storeFile()</pre> in the acceptor stores a copy of the file
		descriptor but doesn't close the original. Which then gets closed after
		storing the second copy. Then two threads extract each its own copy.
		In this example each thread adds its own permission limitation on the
		extracted file handle, one reading, another one writing, even though the
		underlying socket is capable of both reading and writing.
		</para>

		<para>
		Another way is possible if the threads are started sequentially, such as
		if the acceptor thread starts the reader thread, which in turn starts the
		writer thread. Then the reader thread can load the file handle without
		forgetting it in the App before starting the writer thread, and then
		the writer thread would get it and discard from the App:
		</para>

<pre>
# in the acceptor thread
$to->app()->storeCloseFile('name', $socket);
// ... start the reader thread

#----------

# in the reader thread
my ($tf, $socket) = $to->trackDupFile('name', 'r');
// ... start the writer thread

#----------

# in the writer thread
my ($tf, $socket) = $to->trackGetFile('name', 'w');
</pre>

		<para>
		Yet another way is to store the file once and load into
		each thread without forgetting, but then have the storing thread close
		the copy stored in the App
		after all the loading threads have completed the initialization.
		For example:
		</para>

<pre>
# in the acceptor thread
$to->app()->storeCloseFile('name', $socket);
// ... start the reader thread
// ... start the writer thread

$to->readyReady(); // wait for all threads to initialize
$to->app()->closeFd('name');

#----------

# in the reader thread
my ($tf, $socket) = $to->trackDupFile('name', 'r');
$to->readyReady();

#----------

# in the writer thread
my ($tf, $socket) = $to->trackDupFile('name', 'w');
$to->readyReady();
</pre>

		<para>
		In the acceptor thread, <pre>readyReady()</pre> doesn't mark
		the acceptor thread as ready, because it already is, but simply waits
		for all other threads to become ready. And the reader and writer threads
		report that they are ready only after they have copied the file handle
		out of the App. This approach is used, for example, in Triceps::X::ThreadedClient.
		It would not be so great in a server because generally we want the
		server to accept the connections as fast as possible, without waiting
		for each connection's threads to initialize. But it's fine for a client.
		</para>

		<para>
		The App's copy of the file is closed by <pre>closeFd()</pre>. 
		It's an Fd and not a File because the App really stores the OS file
		descriptors, not the Perl file handles, and the File handling is
		done as wrappers on top of it. If you really want, you can deal with the
		raw file descriptors in the App, with the methods described in
		XXXREF App reference
		and
		XXXREF TrieadOwner reference.
		But dealing directly with the Perl file handles is much more convenient.
		</para>
	</sect1>

	<sect1 id="sc_mt_dynamic_server">
		<title>Dynamic threads and fragments in a socket server</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<indexterm>
			<primary>socket</primary>
		</indexterm>
		<para>
		The threads can be used to run a TCP server that
		accepts the connections and then starts the new client communication
		thread(s) for each connection.  This thread can then communicate with
		the rest of the model, feeding and receiving data, as usual, through
		the nexuses.
		</para>

		<para>
		The challenge here is that there must be a way to create the threads
		dynamically, and later when the client closes connection, to dispose of
		them. There are two possible general approaches:
		</para>

		<itemizedlist>
			<listitem>
			Dynamically create and delete the threads in the same App;
			</listitem>

			<listitem>
			Create a new App per connection and connect it to the main App.
			</listitem>
		</itemizedlist>

		<para>
		Both have their own advantages and difficulties, but the approach with
		the dynamic creation and deletion of threads ended up looking easier,
		and that's what Triceps has. The second approach is not particularly
		well supported yet. You can create multiple Apps in one program, and
		you can connect them by making two Triceps Trieads run in the same OS
		thread and ferry the data around. But it's extremely cumbersome. This
		will be improved in the future, but for now the first approach is the
		ticket.
		</para>

		<indexterm>
			<primary>fragment</primary>
		</indexterm>
		<para>
		The dynamically created threads are grouped into the fragments. This is
		done by specifying the fragment name option when creating a thread. The
		threads in a fragment have a few special properties.
		</para>

		<para>
		One, it's possible to shut down (i.e. request to die) the whole fragment in one fell swoop.
		There is no user-accessible way to shut down the individual threads,
		you can shut down either the whole App or a fragment. Shutting down
		individual threads is dangerous, since it can mess up the application
		in many non-obvious ways. But shutting down a fragment is OK, since the
		fragment serves a single logical function, such as servicing one TCP
		connection, and it's OK to shut down the whole logical function.
		</para>

		<para>
		Two, when a thread in the fragment exits, it's really gone, and takes
		all its nexuses with it. Well, technically, the nexuses continue to
		exist and work as long as there are threads connected to them, but no new
		connections can be created after this point. Since usually the whole
		fragment will be gone together, and since the nexuses defined by the
		fragment's thread are normally used only by the other threads of the
		same fragment, a fragment shutdown cleans up its state like the
		fragment had never existed. By contrast, when a normal thread exists,
		the nexuses defined by it stay present and accessible until the App
		shuts down.
		</para>

		<indexterm>
			<primary>chat server</primary>
		</indexterm>
		<para>
		To show how all this stuff works, I've created an example of a <quote>chat
		server</quote>. It's not really a human-oriented chat, it's more of a
		machine-oriented publish-subscribe, and specially tilted to work
		through the running of a socket server with threads.
		</para>

		<para>
		In this case the core logic is absolutely empty. All there is of it, is
		a nexus that passes messages through. The clients read from this
		nexus to get the messages, and write to this nexus to send the
		messages.
		</para>

		<para>
		When the App starts, it has only one thread, the listener thread that
		listens on a socket for the incoming connections. The listener doesn't
		even care about the common nexus and doesn't import it. When a
		connection comes in, the listener creates two threads to serve it: the
		reader reads the socket and sends to the nexus, and the writer receives
		from the nexus and writes to the socket. These two threads constitute a
		fragment for this client. They also create their own private nexus,
		allowing the reader to send control messages to the writer. That could
		also have been done through the central common nexus, but I wanted to
		show that there are different ways of doing things.
		</para>

		<para>
		With a couple of clients connected, threads and sockets start looking
		as shown in
		<xref linkend="fig_mt_chat" xrefstyle="select: label nopage"/>&xrsp;.
		And the listener thread still stays on the side.
		</para>


		<figure id="fig_mt_chat" >
			<title>Chat server internal structure.</title>
			<xi:include href="file:///FIGS/thread-020-chat.xml"/> 
		</figure>

		<para>
		Now let's look at the code, located in <pre>t/xChatMt.t</pre>.
		Let's start with the top-level: how the server gets started. It's
		really the last part of the code, that brings everything together.
		</para>

		<para>
		It uses the ThreadedServer infrastructure:
		</para>

<!-- t/xChatMt.t -->
<pre>
use Triceps::X::ThreadedServer qw(printOrShut);
</pre>

		<indexterm>
			<primary>ThreadedServer</primary>
		</indexterm>
		<para>
		The X subdirectory is for the examples and experimental stuff, but the
		ThreadedServer is really of production quality, I just haven't written
		a whole set of tests for it yet.
		</para>

		<para>
		The server gets started like this:
		</para>

<!-- t/xChatMt.t, shifted -->
<pre>
my ($port, $pid) = Triceps::X::ThreadedServer::startServer(
		app => "chat",
		main => \&listenerT,
		port => 0,
		fork => 1,
);
</pre>

		<para>
		The port option of 0 means <quote>pick any free port</quote>, it will be returned as
		the result.  If you know the fixed port number in advance, use it.
		<quote>chat</quote> will be the name of the App, and <pre>listenerT</pre> is the main function
		of the thread that will listen for the incoming connections and start
		the other threads. And it's also the first thread that gets started, so
		it's responsible for creating the core part of the App as well (though
		in this case there is not a whole lot of it).
		</para>

		<para>
		The option <quote>fork</quote> determines how and whether the server gets started in
		the background. The value 1 means that a new process will be forked,
		and then the threads will be created there. The returned PID can be
		used to wait for that process to complete:
		</para>

<pre>
waitpid($pid, 0);
</pre>

		<para>
		Of course, if you're starting a daemon, you'd probably write this PID
		to a file and then just exit the parent process.
		</para>

		<para>
		The fork value of 0 starts the server in the current process, and the
		current thread becomes the App's harvester thread (the one that
		joins the other threads when the App shuts down).
		</para>

		<para>
		In this case the server doesn't return until it's done, so there is not
		much point in the returned port value, by that time the socket will be
		already closed. In this case you really need to either use a fixed port
		or write the port number to a file from your listener thread. The PID
		also doesn't make sense, and it's returned as <pre>undef</pre>. Here is an example
		of this kind of call:
		</para>

<!-- t/xChatMt.t, shifted -->
<pre>
my ($port, $pid) = Triceps::X::ThreadedServer::startServer(
		app => "chat",
		main => \&listenerT,
		port => 12345,
		fork => 0,
);
</pre>

		<para>
		Finally, the server can be started in the current process, with a new
		thread created as the App's harvester thread, setting the <quote>fork</quote> option 
		to -1. The
		original thread can then continue and do other things in parallel. It's
		the way I use for the unit tests.
		</para>

<!-- t/xChatMt.t, shifted -->
<pre>
my ($port, $thread) = Triceps::X::ThreadedServer::startServer(
		app => "chat",
		main => \&listenerT,
		port => 0,
		fork => -1,
);
</pre>

		<para>
		In this case the second value returned is not a PID but the Perl thread
		object for the harvester thread. You should either detach it or
		eventually join it:
		</para>

<pre>
$thread->join();
</pre>

		<indexterm>
			<primary>error handling</primary>
			<secondary>multithreaded</secondary>
		</indexterm>
		<para>
		Perl propagates the errors in the threads through the <pre>join()</pre>, so if the
		harvester thread dies, that would show only in the <pre>join()</pre> call. And
		since Triceps propagates the errors too, any other App's thread dying will
		cause the harvester thread to die after it shuts down and joins all the App's threads. 
		So if there are any errors in the application, it will die and you will know
		it right away, and not left wondering why does the application appear to run but
		not work right. Unless, of course, you wrap the whole thing in an <pre>eval</pre>,
		then you get the error message and the clean state, and can try running another App.
		</para>

		<para>
		As described before, each socket gets served by two threads: one runs
		reading from the socket and forwards the data into the model and
		another one runs getting data from the model and forwards it into the
		socket. Since the same thread can't wait for both a socket descriptor
		and a thread synchronization primitive, they have to be separate.
		</para>

		<para>
		The first thread started for a connection by the listener is the socket
		reader. Let's go through it bit by bit.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
sub chatSockReadT
{
	my $opts = {};
	&Triceps::Opt::parse("chatSockReadT", $opts, {@Triceps::Triead::opts,
		socketName => [ undef, \&Triceps::Opt::ck_mandatory ],
	}, @_);
	undef @_; # avoids a leak in threads module
	my $owner = $opts->{owner};
	my $app = $owner->app();
	my $unit = $owner->unit();
	my $tname = $opts->{thread};

	# only dup the socket, the writer thread will consume it
	my ($tsock, $sock) = $owner->trackDupFile($opts->{socketName}, "<");
</pre>

		<para>
		The beginning is quite usual. Then it loads the socket from the App and
		gets it tracked with the TrieadOwner. The listener passes the name
		under which it stored the socket in the App as the option
		<quote>socketName</quote>.
		<pre>trackDupSocket()</pre> leaves the socket instance in the App, to be found by the
		writer-side thread.
		</para>

		<para>
		The socket is loaded in this thread as read-only. The writing to the
		socket from all the threads has to be synchronized to avoid mixing the
		half-messages. And the easiest way to synchronize is to always write
		from one thread; if the other thread wants to write something, it
		has to pass the data to the writer thread through the control nexus.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
	# user messages will be sent here
	my $faChat = $owner->importNexus(
		from => "global/chat",
		import => "writer",
	);

	# control messages to the reader side will be sent here
	my $faCtl = $owner->makeNexus(
		name => "ctl",
		labels => [
			ctl => $faChat->impRowType("ctl"),
		],
		reverse => 1, # gives this nexus a high priority
		import => "writer",
	);
</pre>

		<para>
		Imports the chat nexus and creates the private control nexus for
		communication with the writer side. The name of the chat nexus is
		hardcoded here, since it's pretty much a solid part of the application.
		If this were a module, the name of the chat nexus could be passed
		through the options.
		</para>

		<indexterm>
			<primary>Nexus</primary>
			<secondary>reverse</secondary>
		</indexterm>
		<para>
		The control nexus is marked as reverse even though it really isn't. But
		the reverse option has a side effect of making this nexus
		high-priority. Even if the writer thread has a long queue of messages
		from the chat nexus, the messages from the control nexus will be read
		first. Which again isn't strictly necessary here, but I wanted to show
		how it's done.
		</para>

		<para>
		The type of the control label is imported from the chat nexus, so it
		doesn't have to be defined from scratch.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
	$owner->markConstructed();

	Triceps::Triead::start(
		app => $opts->{app},
		thread => "$tname.rd",
		fragment => $opts->{fragment},
		main => \&chatSockWriteT,
		socketName => $opts->{socketName},
		ctlFrom => "$tname/ctl",
	);

	$owner->readyReady();
</pre>

		<indexterm>
			<primary>fragment</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
			<secondary>stages</secondary>
		</indexterm>
		<para>
		Then the construction is done and the writer thread gets started.  And
		then the thread becomes ready and waits for the writer thread to be ready
		too. The <pre>readyReady()</pre> works in the fragments just as it does at the
		start of the App. Whenever a new thread is started, the App becomes not
		ready, and stays this way until all the threads report that they are
		ready. The rest of the App keeps working like nothing happened, at
		least sort of. Whenever a nexus is imported, the messages from this
		nexus start collecting for this thread, and if there are many of them,
		the nexus will become backed up and the threads writing to it will
		block. The new threads have to call <pre>readyReady()</pre> as usual to
		synchronize between themselves, and then everything gets on its way.
		</para>

		<para>
		Of course, if two connections are received in a quick succession, that
		would start two sets of threads, and <pre>readyReady()</pre> will continue only
		after all of them are ready. This is not very good but acceptable in most cases.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
	my $lbChat = $faChat->getLabel("msg");
	my $lbCtl = $faCtl->getLabel("ctl");

	$unit->makeHashCall($lbCtl, "OP_INSERT", 
		cmd => "print", arg => "!ready," . $opts->{fragment});
	$owner->flushWriters();
</pre>

		<para>
		A couple of labels get remembered for the future use, and the
		connection ready message gets sent to the writer thread through the
		control nexus. By convention of this application, the messages go in
		the CSV format, with the control messages starting with <quote>!</quote>. If this is
		the first client, this would send
		</para>

<exdump>
!ready,cliconn1
</exdump>

		<para>
		to the client. It's important to call <pre>flushWriters()</pre> every time to get
		the message(s) delivered.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
	while(<$sock>) {
		s/[\r\n]+$//;
		my @data = split(/,/);
		if ($data[0] eq "exit") {
			last; # a special case, handle in this thread
		} elsif ($data[0] eq "kill") {
			eval {$app->shutdownFragment($data[1]);};
			if ($@) {
				$unit->makeHashCall($lbCtl, "OP_INSERT", cmd => "print", arg => "!error,$@");
				$owner->flushWriters();
			}
		} elsif ($data[0] eq "shutdown") {
			$unit->makeHashCall($lbChat, "OP_INSERT", topic => "*", msg => "server shutting down");
			$owner->flushWriters();
			Triceps::AutoDrain::makeShared($owner);
			eval {$app->shutdown();};
		} elsif ($data[0] eq "publish") {
			$unit->makeHashCall($lbChat, "OP_INSERT", topic => $data[1], msg => $data[2]);
			$owner->flushWriters();
		} else {
			# this is not something you want to do in a real chat application
			# but it's cute for a demonstration
			$unit->makeHashCall($lbCtl, "OP_INSERT", cmd => $data[0], arg => $data[1]);
			$owner->flushWriters();
		}
	}
</pre>

		<para>
		The main loop keeps reading lines from the socket and interpreting
		them. The lines are in CSV format, and the first field is the command
		and the rest are the arguments (if any).  The commands are:
		</para>

		<variablelist>
			<varlistentry>
				<term>publish</term>
				<listitem>
				<para>
				Send a message with a topic to the chat nexus.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>exit</term>
				<listitem>
				<para>
				Close the connection.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>kill</term>
				<listitem>
				<para>
				Close another connection, by name.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>shutdown</term>
				<listitem>
				<para>
				Shut down the server.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>subscribe</term>
				<listitem>
				<para>
				Subscribe the client to a topic.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>unsibscribe</term>
				<listitem>
				<para>
				Unsubscribe the client from a topic.
				</para>
				</listitem>
			</varlistentry>
		</variablelist>

		<para>
		The <quote>exit</quote> just exits the loop, since it works the same as if the socket
		just gets closed from the other side.
		</para>

		<para>
		The <quote>kill</quote> shuts down by name the fragment where the threads of the other
		connection belong. This is a simple application, so it doesn't check
		any permissions, whether this fragment should allowed to be shut down.
		If there is no such fragment, the shutdown call will silently do
		nothing, so the error check and reporting is really redundant (if
		something goes grossly wrong in the thread interruption code, an error
		might still occur, but theoretically this should never happen).
		</para>

		<para>
		The <quote>shutdown</quote> sends the notification to the common topic <quote>*</quote> (to which
		all the clients are subscribed by default), then drains the model and
		shuts it down. The drain makes sure that all the messages in the model
		get processed (and even written to the sockets) without allowing any new
		messages to be injected. <quote>Shared</quote> means that there is no special
		exceptions for some threads.
		</para>

		<para>
		<pre>AutoDrain::makeShared()</pre> actually creates a drain object that keeps the drain
		active during its lifetime. Here this object is not assigned anywhere,
		so it gets immediately destroyed and lifts the drain. So potentially
		more messages can get squeezed in between this point and shutdown.
		Which doesn't matter a whole lot here.
		</para>

		<para>
		If it were really important that nothing get sent after the shutdown
		notification, it could be done like this (this is an untested fragment,
		so it might contain typos):
		</para>

<!-- XXX should make a snippet? -->
<pre>
		} elsif ($data[0] eq "shutdown") {
			my $drain = Triceps::AutoDrain::makeExclusive($owner);
			$unit->makeHashCall($lbChat, "OP_INSERT", 
				topic => "*", msg => "server shutting down");
			$owner->flushWriters();
			$drain->wait();
			eval {$app->shutdown();};
		}
</pre>

		<para>
		This starts the drain, but this time the exclusive mode means that this
		thread is excluded from the drain and allowed to send more data. When the drain is created, it
		waits for success, so when the new message is inserted, it will be
		after all the other messages. <pre>$drain->wait()</pre> does another wait and
		makes sure that this last message propagates all the way. And then the
		app gets shut down, while the drain is still in effect, so no more
		messages can be sent for sure.
		</para>

		<para>
		The <quote>publish</quote> sends the data to the chat nexus (note the <pre>flushWriters()</pre>,
		as usual!).
		</para>

		<para>
		And the rest of commands (that would be <quote>subscribe</quote> and <quote>unsubscribe</quote> but
		you can do any other commands like <quote>print</quote>) get simply forwarded to the
		reader thread for execution. Sending through the commands like this
		without testing is not a good practice for a real application but it's
		cute for a demo.
		</para>

<!-- t/xChatMt.t, chatSockReadT -->
<pre>
	{
		# let the data drain through
		my $drain = Triceps::AutoDrain::makeExclusive($owner);

		# send the notification - can do it because the drain is excluding itself
		$unit->makeHashCall($lbCtl, "OP_INSERT", cmd => "print", arg => "!exiting");
		$owner->flushWriters();

		$drain->wait(); # wait for the notification to drain

		$app->shutdownFragment($opts->{fragment});
	}

	$tsock->close(); # not strictly necessary
}
</pre>

		<para>
		The last part is when the connection get closed, either by the <quote>exit</quote>
		command or when the socket gets closed. Remember, the socket can get
		closed asymmetrically, in one direction, so even when the reading is
		closed, the writing may still work and needs to return the responses to
		any commands received from the socket. And of course the same is true
		for the <quote>exit</quote> command.
		</para>

		<indexterm>
			<primary>drain</primary>
		</indexterm>
		<para>
		So here the full exclusive drain sequence is used, ending with the
		shutdown of this thread's own fragment, which will close the socket.
		Even though only one fragment needs to be shut down, the drain drains
		the whole model. Because of the potentially complex interdependencies,
		there is no way to reliably drain only a part, and all the drains are
		App-wide.
		</para>

		<para>
		The last part, with <pre>$tsock->close()</pre>, is not technically necessary since
		the shutdown of the fragment will get the socket descriptor revoked
		anyway, and then the socket will get closed when the last reference to
		it disappears. But other than that, it's a good practice that unregisters the
		socket from the TrieadOwner and then closes it. 
		</para>
	</sect1>
</chapter>
