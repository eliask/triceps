<?xml version="1.0" encoding="UTF-8"?>

<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5CR3//EN"
	"http://www.oasis-open.org/docbook/xml/4.5CR3/docbookx.dtd" [
<!ENTITY % userents SYSTEM "file:///ENTS/user.ent" >
%userents;
]>

<!--
(C) Copyright 2011-2013 Sergey A. Babkin.
This file is a part of Triceps.
See the file COPYRIGHT for the copyright notice and license information
-->

<chapter id="ch_mt" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Multithreading</title>

	<sect1 id="sc_mt_concepts">
		<title>Triceps multithreading concepts</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<para>
		When running the CEP models, naturally the threads have to be connected
		by the queues for the data exchange. The use of queues is extremely
		popular but also notoriously bug-prone.
		</para>

		<para>
		The idea of the multithreading support in Triceps is to make writing
		the multithreaded model easier. To make writing the good code easy and
		writing the bad code hard. But of course you don't have to use it, if 
		it feels too constraining, you can always make your own.
		</para>

		<para>
		The diagram in
		<xref linkend="fig_mt_overview" xrefstyle="select: label nopage"/>&xrsp;
		shows all the main elements of a multithread Triceps application.
		</para>

		<figure id="fig_mt_overview" >
			<title>Triceps multithreaded application.</title>
			<xi:include href="file:///FIGS/thread-010-over.xml"/> 
		</figure>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<indexterm>
			<primary>fragment</primary>
		</indexterm>
		<para>
		The Triceps application is embodied in the class App. It's possible to
		have multiple Apps in one program.
		</para>

		<para>
		Each thread has multiple parts to it. First, of course, there is the
		OS-level (or, technically, library-level, or Perl-level) thread where
		the code executes. And then there is a class that represents this
		thread and its place in the App. To reduce the naming conflict, this
		class is creatively named Triead (pronounced still <quote>thread</quote>). In the
		discussion I use the word <quote>thread</quote> for both concepts, the OS-level
		thread and the Triead, and it's usually clear from the context which
		one I mean. But sometimes it's particularly important to make the
		distinction, and then I name one or the other explicitly.
		</para>

		<para>
		The class Triead itself is largely opaque, allowing only a few methods
		for introspection. But there is a control interface to it, called
		TrieadOwner. The Triead is visible from the outside, the TrieadOwner
		object is visible only in the OS thread that owns the Triead. The
		TrieadOwner manages the thread state and acts as the intermediary in
		the thread's communications with the App.
		</para>

		<para>
		The data is passed between the threads through the Nexuses. A Nexus is
		unidirectional, with data going only one way, however it may have
		multiple writers and multiple readers. All the readers see the exact
		same data, with rowops going in the exact same order (well, there will
		be other policies in the future as well, but for now there is only one
		policy).
		</para>

		<para>
		A Nexus passes through the data for multiple labels, very much like an
		FnReturn does (and indeed there is a special connection between them).
		A Nexus also allows to export the row types and table types from one
		thread to another.
		</para>

		<para>
		A Nexus is created by one thread, and then the other threads connect to it.
		The thread that creates the Nexus determines what labels will it contain,
		and what row types and table types to export.
		</para>

		<para>
		A Nexus gets connected to the Trieads through the Facets (in the diagram,
		the Facets are shown as flat spots on the round Nexuses). A Facet is
		a connection point between the Nexus and the Triead. Each Facet is for
		either reading or writing. And there may be only one Facet between a
		given Nexus and a given Triead, you can't make multiple connections
		between them. As a consequence, a thread can't both write and read to
		the same Nexus, it can do only one thing. This might actually be an
		overly restrictive limitation and might change in the future but that's
		how things work now.
		</para>

		<para>
		Each Nexus also has a direction: either direct (<quote>downwards</quote>) or reverse
		(<quote>upwards</quote>). How does it know, which direction is down and
		whih is up? It doesn't. You tell it by designating a Nexus one way or the other.
		And yes, the reverse Nexuses allow to build the models
		with loops. However the loops consisting of only the direct Nexuses are
		not allowed, nor of only reverse Nexuses. They would mess up the flow
		control. The proper loops must contain a mix of direct and reverse
		Nexuses.
		</para>

		<para>
		The direct Nexuses have a limited queue size and stop the writers when
		the queue fills up, until the data gets consumed, thus providing the
		flow control. The reverse Nexuses have an unlimited queue size, which
		allows to avoid the circular deadlocks. The reverse Nexuses also have
		a higher priority: if a thread is reading from a direct Nexus and a
		reverse one, with both having data available, it will read the data
		from the reverse Nexus first. This is to prevent the unlimited queues
		in the reverse Nexuses from the truly unlimited growth.
		</para>

		<para>
		Normally an App is built once and keeps running in this configuration
		until it stops. But there is a strong need to have the threads
		dynamically added and deleted too. For example, if the App running as a
		server, and clients connect to it, each client needs to have its
		thread(s) added when the client connects and then deleted when the client
		disconnects. This is handled through the concept of fragments. There is
		no Fragment class but when you create a Triead, you can specify a
		fragment name for it. Then it becomes possible to shut down and dispose
		the threads in a fragment after the fragment's work is done. 
		</para>
	</sect1>

	<sect1 id="sc_mt_triead_life">
		<title>The Triead lifecycle</title>

		<indexterm>
			<primary>Triead</primary>
			<secondary>stages</secondary>
		</indexterm>

		<para>
		Each Triead goes through a few stages in its life:
		</para>

		<itemizedlist>
			<listitem>
			declared
			</listitem>
			<listitem>
			defined
			</listitem>
			<listitem>
			constructed
			</listitem>
			<listitem>
			ready
			</listitem>
			<listitem>
			waited ready
			</listitem>
			<listitem>
			requested dead
			</listitem>
			<listitem>
			dead
			</listitem>
		</itemizedlist>

		<para>
		Note by the way that it's the stages of the Triead object. The OS-level
		thread as such doesn't know much about them, even though these stages
		do have some connections to its state.
		</para>

		<para>
		These stages always go in order and can not be skipped. However for
		convenience you can request a move directly to a further stage. This will just
		automatically pass through all the intermediate stages. Although, well,
		there is one exception: the <quote>waited ready</quote> and <quote>requested dead</quote> stages
		can get skipped on the way to <quote>dead</quote>. Other than that, there is always
		the sequence, so if you find out that a Triead is dead, you can be sure
		that it's also declared, defined, constructed and ready. The attempts
		to go to a previous stage are silently ignored.
		</para>

		<para>
		Now, what do these stages mean?
		</para>

		<variablelist>

			<varlistentry>
				<term>Declared:</term>
				<listitem>
				<para>
				The App knows the name of the thread and that this thread
				will eventually exist. When an App is asked to find the resources from
				this thread (such as Nexuses, and by the way, the Nexuses are
				associated with the threads that created them) it will know to wait
				until this thread becomes constructed, and then look for the resources.
				It closes an important race condition: the code that defines the Triead
				normally runs in a new OS thread but there is no way to tell when
				exactly will it run and do its work. If you had spawned a new thread and
				then attempted to get a nexus from it before it actually runs, the App
				would tell you that there is no such thread and fail. To get around it,
				you declare the thread first and then start it. Most of the time there
				is no need to declare explicitly, the library code that wraps the
				thread creation does it for you.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Defined:</term>
				<listitem>
				<para>
				The Triead object has been created and connected to the App.
				Since this is normally done from the new OS thread, it also implies
				that the thread is running and is busy about constructing the nexuses
				and whatever its own internal resources.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Constructed:</term>
				<listitem>
				<para>
				The Triead had constructed and exported all the nexuses
				that it planned to. This means that now these nexuses can be imported
				by the other threads (i.e. connected to the other threads). After this
				point the thread can not construct any more nexuses. However it can
				keep importing the nexuses from the other threads. It's actually a good
				idea to do all your exports, mark the thread constructed, and only then
				start importing. This order guarantees the absence of initialization deadlocks (which
				would be detected and will cause the App to be aborted). There are some
				special cases when you need to import a nexus from a thread that is not
				fully constructed yet, and it's possible, but requires more attention
				and a special override. I'll talk about it in more detail in 
				XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Ready:</term>
				<listitem>
				<para>
				The thread had imported all the nexuses it wanted and fully
				initialized all its internals (for example, if it needs to load data
				from a file, it might do that before telling that it's ready). After
				this point no more nexuses can be imported. A fine point is that the
				other threads may still be created, and they may do their exporting and
				importing, but once a thread is marked as ready, it's cast in bronze.
				And in the simple cases you don't need to worry about separating the
				constructed and ready stages, just initialize everything and mark the
				thread as ready.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Waited ready:</term>
				<listitem>
				<para>
				Before proceeding further, the thread has to wait for all
				the threads in App to be ready, or it would lose data when it tries to
				communicate with them. It's essentially a barrier. Normally both the
				stages <quote>ready</quote> and <quote>waited ready</quote> are advanced to with a single call
				<pre>readyReady()</pre>. With it the thread says <quote>I'm ready, and let me continue when
				everyone is ready</quote>. After that the actual work can begin. It's still
				possible to create more threads after that (normally, parts of the
				transient fragments), and until they all become ready, the App may
				temporarily become unready again, but that's a whole separate advanced
				topic that will be discussed in XXXREF.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Requested dead:</term>
				<listitem>
				<para>
				This is the way to request a thread to exit. Normally
				some control thread will decide that the App needs to exit and will
				request all its threads to die. The threads will get these requests,
				perform their last rites and exit. The threads don't have to get this
				request to exit, they can also always decide to exit on their own. When
				a thread is requested to die, all the data communication with it stops.
				No more data will get to it through the nexuses and any data it sends
				will be discarded. It might churn a little bit through the data in its
				input buffers but any results produced will be discarded. The good
				practice is to make sure that all the data is drained before requesting
				a thread to die. Note that the nexuses created by this thread aren't
				affected at all, they keep working as usual. It's the data connections
				between this thread and any nexuses that get broken.
				</para>
				</listitem>
			</varlistentry>

			<varlistentry>
				<term>Dead:</term>
				<listitem>
				<para>
				The thread had completed its execution and exited. Normally you
				don't need to mark this explicitly. When the thread's main function
				returns, the library will do it for you. Marking the thread dead also
				drives the harvesting of the OS threads: the harvesting logic will
				perform a <pre>join()</pre> (not to be confused with SQL join) of the thread and
				thus free the OS resources. The dead Trieads are still visible in the
				App (except for some special cases with the fragments), and their
				nexuses continue working as usual (even including the special cases
				with the fragments), the other threads can keep communicating through
				them for as long as they want. 
				</para>
				</listitem>
			</varlistentry>
		</variablelist>
	</sect1>

	<sect1 id="sc_mt_pipeline">
		<title>Multithreaded pipeline</title>

		<indexterm>
			<primary>pipeline</primary>
		</indexterm>
		<para>
		The multithreaded models are well suited for running the pipelines,
		so that is going to be the first example of the threads. The full text of the example
		can be found in <pre>t/xTrafficAggMt.t</pre> in the class Traffic1.
		It's a variation of an already shown example, the traffic
		data aggregation from 
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;.
		The short recap is that it gets
		the data for each network packet going through and keeps it for some
		time, aggregates the data by the hour and keeps it for a longer time,
		and aggregates it by the day and keeps for a longer time yet. This
		multi-stage computation naturally matches the pipeline approach.
		</para>

		<para>
		Since this new example highlights different features than the original one,
		I've changed it logic a little: it updates both the
		hourly and daily summaries on every packet received. And I didn't
		bother to implement the part with the automatic cleaning of the old
		data, it doesn't add anything interesting to the pipeline works.
		</para>

		<para>
		The pipeline topologies are quite convenient for working with the
		threads. The parallel computations create a possibility of things
		happening in an unpredictable order and producing unpredictable
		results. The pipeline topology allows the parallelism and at the same
		time also keeps the data in the same predictable order, with no
		possibility of rows overtaking each other.
		</para>

		<para>
		The computation in this example is split into the following threads:
		</para>

		<itemizedlist>
			<listitem>
			Read the input, parse and send the data into the model.
			</listitem>

			<listitem>
			Store the recent data and aggregate it by the hour.
			</listitem>

			<listitem>
			Store the hourly data and aggregate it by the day.
			</listitem>

			<listitem>
			Store the daily data.
			</listitem>

			<listitem>
			Get the data at the end of the pipeline and print it.
			</listitem>
		</itemizedlist>

		<para>
		The result of each aggregation gets stored in a table in the next thread,
		which then uses the same table for the next stage of aggregation.
		</para>

		<para>
		Technically, each stage only needs the data from the previous
		stage, but to get the updates to the printing stage (since we want
		to print the original updates, daily and hourly), they all go all
		the way through.
		</para>

		<para>
		Dumping the contents of the tables also requires some special support.
		Each table is local to its thread and can't be accessed from the other
		threads. To dump its contents, the dump request needs to be sent to its
		thread, which would extract the data and send it through. There are
		multiple ways to deal with the dump results. One is to have a special
		label for each table's dump and propagate it to the last stage to
		print. If all that is needed is text, another way is to have one label that allows to send
		strings is good enough, all the dumps can send the data converted to
		text into it, and it would go all the way through the pipeline.
		For this example I've picked the last approach.
		</para>

		<para>
		And now is time to show some code. The main part goes like this:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
Triceps::Triead::startHere(
	app => "traffic",
	thread => "print",
	main => \&printT,
);
</pre>

		<indexterm>
			<primary>App</primary>
		</indexterm>
		<indexterm>
			<primary>Triead</primary>
		</indexterm>
		<para>
		The <pre>startHere()</pre> creates an App and starts a Triead in the current OS
		thread. <quote>Here</quote> in the method name stands for <quote>in the current OS thread</quote>.
		<quote>traffic</quote> is the app name, <quote>print</quote> the thread name. This thread
		will be the end of the pipeline, and it will create the rest of the
		threads. This is a convenient pattern when the results of the model
		need to be fed back to the current thread, and it works out very
		conveniently for the unit tests. <pre>printT()</pre> is the body function of
		this printing thread:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub printT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	Triceps::Triead::start(
		app => $opts->{app},
		thread => "read",
		main => \&readerT,
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "raw_hour",
		main => \&rawToHourlyT,
		from => "read/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "hour_day",
		main => \&hourlyToDailyT,
		from => "raw_hour/data",
	);
	Triceps::Triead::start(
		app => $opts->{app},
		thread => "day",
		main => \&storeDailyT,
		from => "hour_day/data",
	);

	my $faIn = $owner->importNexus(
		from => "day/data",
		as => "input",
		import => "reader",
	);

	$faIn->getLabel("print")->makeChained("print", undef, sub {
		&send($_[1]->getRow()->get("text"));
	});
	for my $tag ("packet", "hourly", "daily") {
		makePrintLabel($tag, $faIn->getLabel($tag));
	}

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		<pre>startHere()</pre> accepts a number of fixed options plus arbitrary options
		that it doesn't care about by itself but passes through to the thread's main
		function, which are then the responsibility of the main function to
		parse. To reiterate, the main function gets all the options from the
		call of <pre>startHere()</pre>, both these that <pre>startHere()</pre> parses and these that
		it simply passes through. <pre>startHere()</pre> also adds one more option on its
		own: <quote>owner</quote> containing the TrieadOwner object that the thread uses to
		communicate with the rest of the App.
		</para>

		<para>
		In this case <pre>printT()</pre> doesn't have any extra options on its own,
		it's just happy to get <pre>startHere()</pre>'s standard set that it takes all
		together from <pre>@Triceps::Triead::opts</pre>.
		</para>

		<indexterm>
			<primary>TrieadOwner</primary>
		</indexterm>
		<indexterm>
			<primary>Unit</primary>
		</indexterm>
		<indexterm>
			<primary>UnitClearingTrigger</primary>
		</indexterm>
		<para>
		It gets the TrieadOwner object <pre>$owner</pre> from the option appended by
		<pre>startHere()</pre>. Each TrieadOwner is created with its own Unit, so the unit
		is obtained from it to create the thread's model in it. Incidentally,
		the TrieadOwner  also acts as a clearing trigger object for the Unit,
		so when the TrieadOwner is destroyed, it properly clears the Unit.
		</para>

		<para>
		Then it goes and creates all the threads of the pipeline. The <pre>start()</pre>
		works very much like <pre>startHere()</pre>, only it actually creates a new thread
		and starts the main function in it. The main function can be the same
		whether it runs through <pre>start()</pre> or <pre>startHere()</pre>. The special catch is
		that the options to <pre>start()</pre> must contain only the plain Perl values,
		not Triceps objects. It has to do with how Perl works with threads: it
		makes a copy of every value for the new thread, and it cant's copy the
		XS objects, so they simply become undefined in the new thread.
		</para>

		<para>
		All but the first thread in the pipeline have the extra option <quote>from</quote>: it
		specifies the input nexus for this thread, and each thread creates an
		output nexus <quote>data</quote>. A nexus it named relatively to the
		thread that created it, so when the option <quote>from</quote> says <quote>day/data</quote>, it's
		the nexus <quote>data</quote> created by the thread <quote>day</quote>.
		</para>

		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<para>
		So, the pipeline gets all connected sequentially until eventually
		<pre>printT()</pre> imports the nexus at its tail. <pre>importNexus()</pre> returns a
		Facet, which is the thread's API to the nexus. A facet looks very much
		like an FnReturn for most purposes, with a few additions. It even has a
		real FnReturn in it, and you work with the labels of that FnReturn to get the
		data out of the nexus (or to send data into the nexus). You could potentially
		use an FnBinding with that FnReturn but the typical pattern for reading
		from a facet is different: just get its labels 
		and chain the handling labels directly to them.
		</para>

		<para>
		The option <quote>as</quote> of <pre>importNexus()</pre> gives the name to the facet and to its
		same-named FnReturn (without it the facet would be named the same as
		the short name of the nexus, in this case <quote>data</quote>). The option <quote>import</quote>
		tells whether this thread will be reading or writing the nexus, and
		in this case it's reading.
		</para>

		<para>
		By the time the pipeline gets to the last stage, it has a few
		labels in its facet:
		</para>

		<itemizedlist>
			<listitem>
			<pre>print</pre> - carries the direct text lines to print in its field <pre>text</pre>,
			and its contents gets printed.
			</listitem>

			<listitem>
			<pre>dumprq</pre> - carries the dump requests to the tables, and the printing
			thread doesn't care about it.
			</listitem>

			<listitem>
			<pre>packet</pre> - carries the raw data about the packets.
			</listitem>

			<listitem>
			<pre>hourly</pre> - carries the hourly summaries.
			</listitem>

			<listitem>
			<pre>daily</pre> - carries the daily summaries.
			</listitem>
		</itemizedlist>

		<para>
		The last three get also printed but this time as whole rows.
		</para>

		<para>
		And after everything is connected, the thread both tells that it's
		ready and waits for all the other threads to become ready by calling
		<pre>readyReady()</pre>. Then its the run time, and <pre>mainLoop()</pre> takes care of it:
		it keeps reading data from the nexus and
		processes it until it's told to shutdown.
		The shutdown will be controlled by the file reading
		thread at the start of the pipeline. The processing is done by getting
		the rowops from the nexus and calling them on the appropriate label in
		the facet, which then calls the the labels chained from it, and that
		gets all the rest of the thread's model running.
		</para>

		<para>
		The reader thread drives the pipeline:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<pre>
sub readerT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {@Triceps::Triead::opts}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	my $rtPacket = Triceps::RowType->new(
		time => "int64", # packet's timestamp, microseconds
		local_ip => "string", # string to make easier to read
		remote_ip => "string", # string to make easier to read
		local_port => "int32", 
		remote_port => "int32",
		bytes => "int32", # size of the packet
	);

	my $rtPrint = Triceps::RowType->new(
		text => "string", # the text to print (including \n)
	);

	my $rtDumprq = Triceps::RowType->new(
		what => "string", # identifies, what to dump
	);

	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			packet => $rtPacket,
			print => $rtPrint,
			dumprq => $rtDumprq,
		],
		import => "writer",
	);

	my $lbPacket = $faOut->getLabel("packet");
	my $lbPrint = $faOut->getLabel("print");
	my $lbDumprq = $faOut->getLabel("dumprq");

	$owner->readyReady();

	while(&readLine) {
		chomp;
		# print the input line, as a debugging exercise
		$unit->makeArrayCall($lbPrint, "OP_INSERT", "> $_\n");

		my @data = split(/,/); # starts with a command, then string opcode
		my $type = shift @data;
		if ($type eq "new") {
			$unit->makeArrayCall($lbPacket, @data);
		} elsif ($type eq "dump") {
			$unit->makeArrayCall($lbDumprq, "OP_INSERT", $data[0]);
		} else {
			$unit->makeArrayCall($lbPrint, "OP_INSERT", "Unknown command '$type'\n");
		}
		$owner->flushWriters();
	}

	{
		# drain the pipeline before shutting down
		my $ad = Triceps::AutoDrain::makeShared($owner);
		$owner->app()->shutdown();
	}
}
</pre>

		<para>
		It starts by creating the nexus with the initial set of the labels: for
		the data about the network packets, for the lines to be printed at the
		end of the pipeline and for the dump requests to the tables in the
		other threads. It gets exported for the other threads to import, and
		also imported right back into this thread, for writing. And then the
		setup is done, <pre>readyReady()</pre> is called, and the processing starts.
		</para>

		<para>
		It reads the CSV lines, splits them, makes a decision if it's a data
		line or dump request, and one way or the other sends it into the nexus.
		The data sent to a facet doesn't get immediately forwarded to the
		nexus. It's collected internally in a tray, and then <pre>flushWriters()</pre>
		sends it on. The <pre>mainLoop()</pre> shown in <pre>printT</pre> calls <pre>flushWriters()</pre>
		automatically after every tray it processes from the input. But when
		reading from a file you've got to do it yourself. Of course, it's more
		efficient to send through multiple rows at once, so a smarter
		implementation would check if multiple lines are available from the
		file and send them in larger bundles.
		</para>

		<indexterm>
			<primary>shutdown</primary>
		</indexterm>
		<indexterm>
			<primary>drain</primary>
		</indexterm>
		<para>
		The last part is the shutdown. After the end of file is reached, it's
		time to shut down the application. You can't just shut down it right
		away because there still might be data in the pipeline, and if you shut
		it down, that data will be lost. The right way is to drain the pipeline
		first, and then do the shutdown when the app is drained.
		<pre>AutoDrain::makeShared()</pre> creates a scoped drain: the drain request for
		all the threads is started when this object is created, and the object
		construction completes when the drain succeeds. When the object is
		destroyed, that releases the drain. So in this case the drain succeeds and
		then the app gets shut down.
		</para>

		<para>
		The shutdown causes the <pre>mainLoop()</pre> calls in all the other threads to
		return, and the threads to exit. Then <pre>startHere()</pre> in the first thread
		has the special logic in it that joins all the started threads after
		its own main function returns and before it completes. After that the
		script continues on its way and is free to exit. 
		</para>

		<para>
		The rest of this example might be easier to understand by looking at an
		example of a run first. The lines in bold are the copies of
		the input lines that <pre>readerT()</pre> reads from the input and
		sends into the pipeline, and <pre>printT()</pre> faithfully
		prints.
		</para>

		<para>
		<pre>input.packet</pre> are the rows that reach the <pre>printT</pre> on the <pre>packet</pre> label
		(remember, <quote>input</quote> is the name with which it imports its input nexus).
		<pre>input.hourly</pre> is the data aggregated by the hour intervals (and also by
		the IP addresses, dropping the port information), and <pre>input.daily</pre>
		further aggregates it per day (and again per the IP addresses). The
		timestamps in the hourly and daily rows are truncated to the start
		of the hour or day.
		</para>

		<para>
		And the lines without any prefixes are the dumps of the table contents
		that again reach the <pre>printT()</pre> through the <quote>print</quote> label:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 -->
<exdump>
> new,OP_INSERT,1330886011000000,1.2.3.4,5.6.7.8,2000,80,100
input.packet OP_INSERT time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
> new,OP_INSERT,1330886012000000,1.2.3.4,5.6.7.8,2000,80,50
input.packet OP_INSERT time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
input.hourly OP_DELETE time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="100" 
input.hourly OP_INSERT time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
> new,OP_INSERT,1330889612000000,1.2.3.4,5.6.7.8,2000,80,150
input.packet OP_INSERT time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
> new,OP_INSERT,1330889811000000,1.2.3.4,5.6.7.8,2000,80,300
input.packet OP_INSERT time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
input.hourly OP_DELETE time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="300" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.hourly OP_INSERT time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
input.daily OP_DELETE time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
input.daily OP_INSERT time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
> new,OP_INSERT,1330972411000000,1.2.3.5,5.6.7.9,3000,80,200
input.packet OP_INSERT time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
input.hourly OP_INSERT time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
input.daily OP_INSERT time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> new,OP_INSERT,1331058811000000
input.packet OP_INSERT time="1331058811000000" 
> new,OP_INSERT,1331145211000000
input.packet OP_INSERT time="1331145211000000" 
> dump,packets
time="1330886011000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="100" 
time="1330886012000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="50" 
time="1330889612000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="150" 
time="1330889811000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" local_port="2000" remote_port="80" bytes="300" 
time="1330972411000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" local_port="3000" remote_port="80" bytes="200" 
> dump,hourly
time="1330884000000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="150" 
time="1330887600000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="450" 
time="1330970400000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
> dump,daily
time="1330819200000000" local_ip="1.2.3.4" remote_ip="5.6.7.8" bytes="600" 
time="1330905600000000" local_ip="1.2.3.5" remote_ip="5.6.7.9" bytes="200" 
</exdump>

		<para>
		Note that the order of the lines is completely nice and predictable,
		nothing goes out of order. Each nexus preserves the order of the rows
		put into it, and the fact that there is only one writer per nexus and
		that every thread is fed from only one nexus, avoids the races.
		</para>

		<para>
		Let's look at the thread that performs the aggregation by the hour:
		</para>

<!-- perl/Triceps/t/xTrafficAggMt.t Traffic1 edited -->
<pre>
# compute an hour-rounded timestamp (in microseconds)
sub hourStamp # (time)
{
	return $_[0]  - ($_[0] % (1000*1000*3600));
}

sub rawToHourlyT # (@opts)
{
	my $opts = {};
	Triceps::Opt::parse("traffic main", $opts, {
		@Triceps::Triead::opts,
		from => [ undef, \&Triceps::Opt::ck_mandatory ],
	}, @_);
	my $owner = $opts->{owner};
	my $unit = $owner->unit();

	# The current hour stamp that keeps being updated;
	# any aggregated data will be propagated when it is in the
	# current hour (to avoid the propagation of the aggregator clearing).
	my $currentHour;

	my $faIn = $owner->importNexus(
		from => $opts->{from},
		as => "input",
		import => "reader",
	);

	# the full stats for the recent time
	my $ttPackets = Triceps::TableType->new($faIn->getLabel("packet")->getRowType())
		->addSubIndex("byHour", 
			Triceps::IndexType->newPerlSorted("byHour", undef, sub {
				return &hourStamp($_[0]->get("time")) <=> &hourStamp($_[1]->get("time"));
			})
			->addSubIndex("byIP", 
				Triceps::IndexType->newHashed(key => [ "local_ip", "remote_ip" ])
				->addSubIndex("group",
					Triceps::IndexType->newFifo()
				)
			)
		)
	;

	# type for a periodic summary, used for hourly, daily etc. updates
	my $rtSummary;

	Triceps::SimpleAggregator::make(
		tabType => $ttPackets,
		name => "hourly",
		idxPath => [ "byHour", "byIP", "group" ],
		result => [
			# time period's (here hour's) start timestamp, microseconds
			time => "int64", "last", sub {&hourStamp($_[0]->get("time"));},
			local_ip => "string", "last", sub {$_[0]->get("local_ip");},
			remote_ip => "string", "last", sub {$_[0]->get("remote_ip");},
			# bytes sent in a time period, here an hour
			bytes => "int64", "sum", sub {$_[0]->get("bytes");},
		],
		saveRowTypeTo => \$rtSummary,
	);

	$ttPackets->initialize();
	my $tPackets = $unit->makeTable($ttPackets, "tPackets");

	# Filter the aggregator output to match the current hour.
	my $lbHourlyFiltered = $unit->makeDummyLabel($rtSummary, "hourlyFiltered");
	$tPackets->getAggregatorLabel("hourly")->makeChained("hourlyFilter", undef, sub {
		if ($_[1]->getRow()->get("time") == $currentHour) {
			$unit->call($lbHourlyFiltered->adopt($_[1]));
		}
	});

	# update the notion of the current hour before the table
	$faIn->getLabel("packet")->makeChained("processPackets", undef, sub {
		my $row = $_[1]->getRow();
		$currentHour = &hourStamp($row->get("time"));
		# skip the timestamp updates without data
		if (defined $row->get("bytes")) {
			$unit->call($tPackets->getInputLabel()->adopt($_[1]));
		}
	});

	# The makeNexus default option chainFront => 1 will make
	# sure that the pass-through data propagates first, before the
	# processed data.
	my $faOut = $owner->makeNexus(
		name => "data",
		labels => [
			$faIn->getFnReturn()->getLabelHash(),
			hourly => $lbHourlyFiltered,
		],
		import => "writer",
	);

	my $lbPrint = $faOut->getLabel("print");

	# the dump request processing
	$tPackets->getDumpLabel()->makeChained("printDump", undef, sub {
		$unit->makeArrayCall($lbPrint, "OP_INSERT", $_[1]->getRow()->printP() . "\n");
	});
	$faIn->getLabel("dumprq")->makeChained("dump", undef, sub {
		if ($_[1]->getRow()->get("what") eq "packets") {
			$tPackets->dumpAll();
		}
	});

	$owner->readyReady();
	$owner->mainLoop(); # all driven by the reader
}
</pre>

		<para>
		This function inherits the options from <pre>Triead::start()</pre> as usual and
		adds the option <quote>from</quote> of its own. This option's value is then used as
		the name of nexus to import for reading. The row types of the labels
		from that imported facet are then used to create the table and
		aggregation.
		</para>

		<para>
		The table and aggregation themselves are the same as in
		<xref linkend="sc_time_periodic" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;,
		so I won't go into much detail describing them.  The only big change is
		the use of SimpleAggergator instead of a manually-built one.  The
		filter logic allows to delete the old raw data without propagation of
		the aggregation changes caused by it.
		</para>

		<para>
		Then the output nexus is created. The creation passes through all the
		incoming data, short-circuiting the input and output, and adds the
		extra label for the aggregated output. The call
		<pre>$faIn->getFnReturn()->getLabelHash()</pre> pulls all the labels
		and their names from the input facet, convenient for passing the
		data directly through to the output.  Just like an FnReturn,
		the Facet construction with <pre>makeNexus()</pre> has the option
		<quote>chainFront</quote> set to 1 by default, and thus when it chains
		the labels from the pass-through ones, they are chained on the front.
		This works very nicely: this way the input data passes through first
		and only then the input goes to the computational labels and produces
		the results that follow it into the output facet.
		</para>

		<para>
		The table dump is implemented after the output facet is defined because
		it needs the print label from that facet to send the results to.
		That print label ends up with two sources of datra for it. One is
		the eponymous label from the input facet, that passes the print
		requests from the previous stage of the pipeline. Another one is the
		table dump logic from this thread. Both are fine and can be mixed
		together.
		</para>

		<para>
		And after that it's all usual <pre>readyReady()</pre> and <pre>mainLoop()</pre>.
		</para>

		<para>
		The <pre>hourlyToDailyT()</pre> is very similar, so I won't even show it here, you can
		find the full text in the sources.
		</para>
	</sect1>

	<sect1 id="sc_mt_objects">
		<title>Object passing between threads</title>

		<indexterm>
			<primary>multithreading</primary>
		</indexterm>
		<indexterm>
			<primary>XS</primary>
		</indexterm>
		<para>
		A limitation of the Perl threads is that no variables can be shared
		between them. Well, there are the special shared variables but
		they're are very special and have great many limitations on their own.
		When a new thread gets created, it gets a copy of all the
		variables of the parent. That is, of all the plain Perl variables. With
		the XS extensions your luck may vary: the variables might get copied,
		might become undefined, or just become broken (if the XS module is not
		threads-aware). Copying the XS variables requires a quite high overhead
		at all the other times, so Triceps doesn't do it and all the Triceps
		object become undefined in the new thread.
		</para>

		<para>
		This model of behavior for a package is marked by creating the
		method <pre>CLONE_SKIP</pre> in it:
		</para>

<pre>
sub CLONE_SKIP { 1; }
</pre>

		<para>
		All the Triceps packages define it, and it's the best practice to
		define it in your packages as well.
		</para>

		<para>
		However the threads are useless without communication, and
		Triceps provides a way to pass around certain objects through the
		Nexuses.
		</para>

		<indexterm>
			<primary>Nexus</primary>
		</indexterm>
		<indexterm>
			<primary>Facet</primary>
		</indexterm>
		<indexterm>
			<primary>Rowop</primary>
		</indexterm>
		<para>
		First, obviously, the Nexuses are intended to pass through the Rowops.
		These Rowops coming out of a nexus are not the same Rowop objects that
		went in. Rowop is a single-threaded object and can not be shared by two
		threads. Instead it gets converted to an internal form while in the
		nexus, and gets re-created when it comes out, pointing to the same Row object and to the
		correct Label in the local Facet.
		</para>

		<para>
		Then, again obviously, the Facets get imported to the other threads
		as an interface of the Nexus, together with their row types.
		</para>

		<indexterm>
			<primary>RowType</primary>
		</indexterm>
		<indexterm>
			<primary>TableType</primary>
		</indexterm>
		<para>
		And two more types of objects can be exported through a Nexus: the
		RowTypes and TableTypes. They get exported through the options as in
		this example:
		</para>

<pre>
$fa = $owner->makeNexus(
    name => "nx1",
    labels => [
        one => $rt1,
        two => $lb,
    ], 
    rowTypes => [
        one => $rt2,
        two => $rt1,
    ], 
    tableTypes => [
        one => $tt1,
        two => $tt2,
    ], 
    import => "writer",
); 
</pre>

		<para>
		As you can see, the namespaces for the labels, row types and table
		types are completely independent, and the same names can be reused in
		each of them for different meaning. All the three sections are
		optional, so if you want, you can export only the types in the nexus,
		without any labels.
		</para>

		<para>
		They can then be extracted from the imported facet as:
		</para>

<pre>
$rt1 = $fa->impRowType("one");
$tt1 = $fa->impTableType("one");
</pre>

		<para>
		Or the whole set of name-value pairs can be obtained with:
		</para>

<pre>
@rtset = $fa->impRowTypesHash();
@ttset = $fa->impTableTypesHash();
</pre>

		<para>
		The exact table types and row types (by themselves or in the table
		types or labels) in the importing thread will be copied. It's
		technically possible to share the references to the same row type
		from multiple threads in
		the &Cpp; code but it's more efficient to make a separate copy for each
		thread, and thus the Perl API goes along the more efficient way.
		</para>

		<para>
		The import is smart in the sense that it preserves the sameness of the
		row types: if in the exporting thread the same row type was referred
		from multiple places in the <pre>labels</pre>, <pre>rowTypes</pre> and <pre>tableTypes</pre> sections,
		in the imported facet that would again be the same row type object (even
		though of course not the one that has been exported but its copy). This
		again helps with the efficiency when various objects decide if the rows
		created by this and that type are matching.
		</para>

		<indexterm>
			<primary>TableType</primary>
		</indexterm>
		<para>
		This is all well until you want to export a table type that has an
		index with a Perl sort condition in it, or an aggregator with the Perl
		code. The Perl code objects are tricky: they get copied OK when a new
		thread is created but the attempts to import them through a nexus later
		cause a terrible memory corruption. So Triceps doesn't allow to export
		the table types with the function references in them. 
		But it provides an
		alternative solution: the code snippets can be specified as the source
		code, as described in
		<xref linkend="sc_code" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;. 
		They get compiled when the table type gets initialized. When a
		table type gets imported through a nexus, it brings the source code
		with it. The imported table types are always uninitialized, so at
		initialization time the source code gets compiled in the new thread and
		works.
		</para>

		<para>
		It all works transparently: just specify a string instead of a function
		reference when creating the index, and it will be recognized and
		processed. For example:
		</para>

<pre>
$it= Triceps::IndexType->newPerlSorted("b_c", undef, '
    my $res = ($_[0]->get("b") <=> $_[1]->get("b")
        || $_[0]->get("c") <=> $_[1]->get("c"));
    return $res;
    '
);
</pre>

		<para>
		Before the code gets compiled, it gets wrapped into a <pre>sub { ... }</pre>, so
		don't write your own <pre>sub</pre> in the code string, that would be an error.
		</para>

		<para>
		To recap the differences between the code references and the
		source code snippets format:
		</para>

		<para>
		When you compile a function, it carries with it the lexical context. So
		you can make the closures that refer to the <quote>my</quote> variables in their
		lexical scope. With the source code you can't do this. The table type
		compiles them at initialization time in the context of the main
		package, and that's all they can see. Remember also that the global
		variables are not shared between the threads, so if you refer to a
		global variable in the code snippet and rely on a value in that
		variable, it won't be present in the other threads (unless the other
		threads are direct descendants and the value was set before their
		creation).
		</para>

		<para>
		There is also the issue of arguments that can be specified for these
		functions. Triceps is smart enough to handle the arguments that are
		one of:
		</para>

		<itemizedlist>
			<listitem>
			<pre>undef</pre>
			</listitem>
			<listitem>
			integer
			</listitem>
			<listitem>
			floating-point
			</listitem>
			<listitem>
			string
			</listitem>
			<listitem>
			Triceps::RowType object
			</listitem>
			<listitem>
			Triceps::Row object
			</listitem>
			<listitem>
			reference to an array or hash thereof
			</listitem>
		</itemizedlist>

		<para>
		It converts the data to an internal &Cpp; representation in the nexus and
		then converts it back on import. So, if a TableType has all the code in
		it in the source form, and the arguments for this code within the
		limits of this format, it can be exported through the nexus. Otherwise
		an attempt to export it will fail.
		</para>

		<indexterm>
			<primary>SimpleOrderedIndex</primary>
		</indexterm>
		<indexterm>
			<primary>aggregation</primary>
		</indexterm>
		<para>
		The SimpleOrderedIndex uses the source code format for the
		functions it generates, so they will pass through the nexuses.
		And if you specify the aggregator functions as code snippets, you
		can export the table types with them through the nexuses too.
		</para>

		<indexterm>
			<primary>SimpleAggregator</primary>
		</indexterm>
		<para>
		However things didn't work out so well for the SimpleAggregator.
		I've found that I can't just do it within the current aggregation
		infrastructure. As mentioned in
		<xref linkend="sc_aggregation_optimized" xrefstyle="select: label quotedtitle pageabbrev"/>&xrsp;,
		the aggregators don't have the same kind of initialization function
		as indexes (one that would run at the table type initialization
		time), and that becomes the deal-breaker. 
		</para>

		<para>
		Fortunately, some thinking had showed that this feature is not really
		needed. There usually just isn't any need to export a table type with
		aggregators. 
		So it's a nice feature to have overall but not urgent.
		Moreover, there is a need to export the table types with
		many elements stripped. 
		</para>

		<para>
		What is to be stripped and why?
		The most central part of the table type is its primary index. It
		defines how the data gets organized. And then the secondary indexes and
		aggregators perform the computations from the data in the table. The
		tables can not be shared between threads, and thus the way to copy a
		table between the threads is to export the table type and send the
		data, then let the other thread construct a copy of the table from that.
		But the table created in another thread really needs only the base data
		organization. If it does any computations on that data, that would be
		its own computations, different than the ones in the exporting thread.
		So all it needs to get is the basic table type with the primary index,
		very rarely some secondary indexes, and pretty much never the
		aggregators. The importing thread would then add its own secondary
		indexes and aggregators before initializing its table type and
		constructing the table from it.
		</para>

		<para>
		The way to get such a stripped table type with only the fundamentally
		important parts is:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental();
</pre>

		<para>
		That copies the row type and the primary index (the whole path to the
		first leaf index type) and leaves alone the rest. All the aggregators
		on all the indexes, even on the primary one, are not included in the
		copy. In the context of the full nexus, making it can look like:
		</para>

<pre>
$facet = $owner->makeNexus(
    name => "data"
    labels => [ @labels ],
    tableTypes => [
         mytable => $mytable->getType()->copyFundamental(),
    ],
    import => "writer",
);
</pre>

		<para>
		In case if more index types need to be included, they can be specified
		by path in the arguments of <pre>copyFundamental()</pre>:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
    [ "byDate", "byAddress", "fifo" ],
    [ "byDate", "byPriority", "fifo" ],
);
</pre>

		<para>
		The paths may overlap, as shown here, and the matching subtrees will be
		copied correctly, still properly overlapping in the result. There is
		also a special syntax:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
    [ "secondary", "+" ],
);
</pre>

		<para>
		The <pre>"+"</pre> in the path means <quote>do the path to the first leaf index of that
		subtree</quote> and saves the necessity to write out the whole path.
		</para>

		<para>
		Finally, what if you don't want to include the original primary index
		at all? You can use the string <pre>"NO_FIRST_LEAF"</pre> as the first argument.
		That would skip it. You can still include it by using its explicit
		path, possibly at the other position.
		</para>

		<para>
		For example, suppose that you have a table type with two top-level
		indexes, <quote>first</quote> is the primary index and <quote>second</quote> as secondary, and
		make a copy:
		</para>

<pre>
$tabtype_fundamental = $tabtype->copyFundamental(
     "NO_FIRST_LEAF",
    [ "second", "+" ],
    [ "first", "+" ],
);
</pre>

		<para>
		In the copied table type the index <quote>second</quote> becomes primary and <quote>first</quote>
		secondary.
		</para>
	</sect1>
</chapter>
